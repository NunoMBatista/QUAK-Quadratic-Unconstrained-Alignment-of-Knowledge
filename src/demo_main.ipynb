{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbd8a99",
   "metadata": {},
   "source": [
    "# Demo: Algorithm vs Paper Alignment\n",
    "This notebook constructs a deterministic toy example where a greedy nearest-neighbor (NN) matcher fails but the QUBO-based solver succeeds. We contrast two tiny knowledge graphs in quantum computing—one about canonical algorithms and another about the landmark papers that introduced them. Carefully crafted text attributes trick the NN into the wrong matches, while structural rewards let the QUBO recover the globally consistent alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55520329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "from rdflib.namespace import RDF\n",
    "\n",
    "PROJECT_ROOT = repo_root = Path().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    OUTPUT_DIR,\n",
    "    GAEA_MMD_WEIGHT,\n",
    "    GAEA_STATS_WEIGHT,\n",
    "    GAEA_MAX_ALIGN_SAMPLES,\n",
    ")\n",
    "from src.embedding.generate_embeddings import load_pyg_data_from_ttl, train_gaea_joint\n",
    "from src.qubo_alignment import formulate\n",
    "from src.evaluation.solvers import (\n",
    "    _solve_with_simulated_annealing,\n",
    "    _extract_alignments,\n",
    "    Alignment,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "DEMO_DIR = OUTPUT_DIR / \"demo\"\n",
    "DEMO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEMO_WIKI_PATH = DEMO_DIR / \"kg_algorithms_demo.ttl\"\n",
    "DEMO_ARXIV_PATH = DEMO_DIR / \"kg_papers_demo.ttl\"\n",
    "\n",
    "WIKI_BASE = \"http://demo.local/wiki/\"\n",
    "ARXIV_BASE = \"http://demo.local/arxiv/\"\n",
    "REL_BASE = \"http://demo.local/rel/\"\n",
    "ENTITY_CLASS_URI = URIRef(\"http://demo.local/schema/Entity\")\n",
    "LABEL_PREDICATE = URIRef(\"http://demo.local/schema/label\")\n",
    "ATTRIBUTE_PREDICATE = URIRef(\"http://demo.local/schema/attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa621e",
   "metadata": {},
   "source": [
    "## 1. Define Toy Algorithm/Paper Graphs\n",
    "The JSON blocks below describe two hand-crafted knowledge graphs. KG1 captures four flagship quantum algorithms; KG2 lists five seminal papers (plus an extra \"Qubit\" resource node). The traps live in the text attributes: VQE carries the QAOA paper title and QAOA carries the VQE title, so any similarity model that greedily trusts text will swap their alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb68075",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_WIKI_COUNT = 4\n",
    "EXPECTED_ARXIV_COUNT = 5\n",
    "\n",
    "DEFAULT_WIKI_ENTITIES: Dict[str, Any] = {\n",
    "    \"Shor's Algorithm\": {\n",
    "        \"name\": \"Shor's Algorithm\",\n",
    "        \"category\": \"Quantum Algorithm\",\n",
    "        \"year\": 1994,\n",
    "    },\n",
    "    \"Grover's Algorithm\": {\n",
    "        \"name\": \"Grover's Algorithm\",\n",
    "        \"category\": \"Quantum Algorithm\",\n",
    "        \"year\": 1996,\n",
    "    },\n",
    "    \"VQE (Variational Quantum Eigensolver)\": {\n",
    "        \"paper_title\": \"A quantum approximate optimization algorithm\",\n",
    "        \"category\": \"Hybrid Algorithm\",\n",
    "        \"year\": 2014,\n",
    "    },\n",
    "    \"QAOA (Quantum Approximate Optimization Algorithm)\": {\n",
    "        \"paper_title\": \"A variational eigenvalue solver on a quantum processor\",\n",
    "        \"category\": \"Hybrid Algorithm\",\n",
    "        \"year\": 2014,\n",
    "    },\n",
    "}\n",
    "\n",
    "DEFAULT_ARXIV_ENTITIES: Dict[str, Any] = {\n",
    "    \"Shor, 1994\": {\n",
    "        \"name\": \"Shor, 1994\",\n",
    "        \"venue\": \"FOCS\",\n",
    "        \"year\": 1994,\n",
    "    },\n",
    "    \"Grover, 1996\": {\n",
    "        \"name\": \"Grover, 1996\",\n",
    "        \"venue\": \"STOC\",\n",
    "        \"year\": 1996,\n",
    "    },\n",
    "    \"Peruzzo et al., 2014\": {\n",
    "        \"paper_title\": \"A variational eigenvalue solver on a quantum processor\",\n",
    "        \"venue\": \"Nature Communications\",\n",
    "        \"year\": 2014,\n",
    "    },\n",
    "    \"Farhi et al., 2014\": {\n",
    "        \"paper_title\": \"A quantum approximate optimization algorithm\",\n",
    "        \"venue\": \"arXiv\",\n",
    "        \"year\": 2014,\n",
    "    },\n",
    "    \"Qubit\": {\n",
    "        \"role\": \"Shared resource\",\n",
    "    },\n",
    "}\n",
    "\n",
    "DEFAULT_WIKI_TRIPLES: List[Tuple[str, str, str]] = [\n",
    "    (\"Shor's Algorithm\", \"references\", \"Grover's Algorithm\"),\n",
    "    (\n",
    "        \"VQE (Variational Quantum Eigensolver)\",\n",
    "        \"published_same_year_as\",\n",
    "        \"QAOA (Quantum Approximate Optimization Algorithm)\",\n",
    "    ),\n",
    "    (\n",
    "        \"QAOA (Quantum Approximate Optimization Algorithm)\",\n",
    "        \"published_same_year_as\",\n",
    "        \"VQE (Variational Quantum Eigensolver)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "DEFAULT_ARXIV_TRIPLES: List[Tuple[str, str, str]] = [\n",
    "    (\"Qubit\", \"appears_in\", \"Shor, 1994\"),\n",
    "    (\"Qubit\", \"appears_in\", \"Grover, 1996\"),\n",
    "    (\"Qubit\", \"appears_in\", \"Peruzzo et al., 2014\"),\n",
    "    (\"Qubit\", \"appears_in\", \"Farhi et al., 2014\"),\n",
    "    (\n",
    "        \"Peruzzo et al., 2014\",\n",
    "        \"published_same_year_as\",\n",
    "        \"Farhi et al., 2014\",\n",
    "    ),\n",
    "    (\n",
    "        \"Farhi et al., 2014\",\n",
    "        \"published_same_year_as\",\n",
    "        \"Peruzzo et al., 2014\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "DEFAULT_HYPERPARAMS: Dict[str, Any] = {\n",
    "    \"epochs\": 80,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"node_weight\": 1.0,\n",
    "    \"structural_weight\": 2.5,\n",
    "    \"structural_match_bonus\": 1.0,\n",
    "    \"wiki_penalty\": 2.0,\n",
    "    \"arxiv_penalty\": 2.0,\n",
    "    \"similarity_threshold\": 0.0,\n",
    "    \"num_reads\": 160,\n",
    "    \"mmd_weight\": GAEA_MMD_WEIGHT,\n",
    "    \"stats_weight\": GAEA_STATS_WEIGHT,\n",
    "    \"max_align_samples\": min(GAEA_MAX_ALIGN_SAMPLES, 512),\n",
    "}\n",
    "\n",
    "\n",
    "def to_pretty_json(value: Any) -> str:\n",
    "    \"\"\"Helper to present data structures in editable JSON form.\"\"\"\n",
    "    return json.dumps(value, indent=2, ensure_ascii=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da374dd",
   "metadata": {},
   "source": [
    "## 2. Build Input Interface with ipywidgets\n",
    "The following cell constructs editable text areas and sliders so you can tweak entities, relations, and solver hyperparameters without touching the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623ee7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget construction for toy data editing and solver tuning\n",
    "wiki_entities_editor = ipw.Textarea(\n",
    "    value=to_pretty_json(DEFAULT_WIKI_ENTITIES),\n",
    "    layout=ipw.Layout(width=\"100%\", height=\"240px\"),\n",
    "    placeholder=\"[\\n  \\\"Quantum Computer\\\",\\n  \\\"Superconducting Qubit\\\"\\n]\",\n",
    ")\n",
    "arxiv_entities_editor = ipw.Textarea(\n",
    "    value=to_pretty_json(DEFAULT_ARXIV_ENTITIES),\n",
    "    layout=ipw.Layout(width=\"100%\", height=\"240px\"),\n",
    "    placeholder=\"[\\n  \\\"Quantum processors\\\",\\n  \\\"Superconducting circuits\\\"\\n]\",\n",
    ")\n",
    "\n",
    "wiki_triples_editor = ipw.Textarea(\n",
    "    value=to_pretty_json(DEFAULT_WIKI_TRIPLES),\n",
    "    layout=ipw.Layout(width=\"100%\", height=\"240px\"),\n",
    "    placeholder=\"[\\n  [\\\"Quantum Computer\\\", \\\"implements\\\", \\\"Quantum Algorithm\\\"]\\n]\",\n",
    ")\n",
    "arxiv_triples_editor = ipw.Textarea(\n",
    "    value=to_pretty_json(DEFAULT_ARXIV_TRIPLES),\n",
    "    layout=ipw.Layout(width=\"100%\", height=\"240px\"),\n",
    "    placeholder=\"[\\n  [\\\"Quantum processors\\\", \\\"implements\\\", \\\"Search routines\\\"]\\n]\",\n",
    ")\n",
    "\n",
    "epochs_slider = ipw.IntSlider(\n",
    "    value=int(DEFAULT_HYPERPARAMS[\"epochs\"]),\n",
    "    min=20,\n",
    "    max=400,\n",
    "    step=20,\n",
    "    description=\"Epochs\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "learning_rate_slider = ipw.FloatLogSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"learning_rate\"]),\n",
    "    base=10,\n",
    "    min=-4,\n",
    "    max=-1,\n",
    "    step=0.1,\n",
    "    description=\"LR\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "node_weight_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"node_weight\"]),\n",
    "    min=0.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description=\"H_node\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "structural_weight_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"structural_weight\"]),\n",
    "    min=0.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description=\"H_struct\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "structural_match_bonus_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"structural_match_bonus\"]),\n",
    "    min=0.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description=\"Match bonus\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "wiki_penalty_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"wiki_penalty\"]),\n",
    "    min=0.1,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description=\"Wiki pen.\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "arxiv_penalty_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"arxiv_penalty\"]),\n",
    "    min=0.1,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description=\"Arxiv pen.\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "similarity_threshold_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"similarity_threshold\"]),\n",
    "    min=0.0,\n",
    "    max=0.95,\n",
    "    step=0.01,\n",
    "    description=\"Sim thresh\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "num_reads_slider = ipw.IntSlider(\n",
    "    value=int(DEFAULT_HYPERPARAMS[\"num_reads\"]),\n",
    "    min=10,\n",
    "    max=1000,\n",
    "    step=10,\n",
    "    description=\"Num reads\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "mmd_weight_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"mmd_weight\"]),\n",
    "    min=0.0,\n",
    "    max=2.0,\n",
    "    step=0.05,\n",
    "    description=\"MMD λ\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "stats_weight_slider = ipw.FloatSlider(\n",
    "    value=float(DEFAULT_HYPERPARAMS[\"stats_weight\"]),\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description=\"Stats λ\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "max_samples_slider = ipw.IntSlider(\n",
    "    value=int(DEFAULT_HYPERPARAMS[\"max_align_samples\"]),\n",
    "    min=128,\n",
    "    max=4096,\n",
    "    step=128,\n",
    "    description=\"MMD samples\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "build_graphs_button = ipw.Button(\n",
    "    description=\"Write TTL\",\n",
    "    button_style=\"info\",\n",
    "    icon=\"upload\",\n",
    "    tooltip=\"Serialize the current toy graphs to output/demo\",\n",
    ")\n",
    "run_alignment_button = ipw.Button(\n",
    "    description=\"Run alignment\",\n",
    "    button_style=\"success\",\n",
    "    icon=\"play\",\n",
    "    tooltip=\"Train the joint GAEA model and compare QUBO vs NN\",\n",
    ")\n",
    "\n",
    "inputs_column = ipw.VBox(\n",
    "    [\n",
    "        ipw.HTML(\"<b>Wiki entities</b>\"),\n",
    "        wiki_entities_editor,\n",
    "        ipw.HTML(\"<b>Wiki triples</b>\"),\n",
    "        wiki_triples_editor,\n",
    "    ],\n",
    "    layout=ipw.Layout(width=\"50%\", padding=\"0 8px 0 0\"),\n",
    ")\n",
    "outputs_column = ipw.VBox(\n",
    "    [\n",
    "        ipw.HTML(\"<b>ArXiv entities</b>\"),\n",
    "        arxiv_entities_editor,\n",
    "        ipw.HTML(\"<b>ArXiv triples</b>\"),\n",
    "        arxiv_triples_editor,\n",
    "    ],\n",
    "    layout=ipw.Layout(width=\"50%\", padding=\"0 0 0 8px\"),\n",
    ")\n",
    "\n",
    "text_input_panel = ipw.HBox([inputs_column, outputs_column], layout=ipw.Layout(width=\"100%\"))\n",
    "\n",
    "slider_row_one = ipw.HBox(\n",
    "    [epochs_slider, learning_rate_slider, mmd_weight_slider, stats_weight_slider],\n",
    "    layout=ipw.Layout(width=\"100%\", justify_content=\"space-between\"),\n",
    ")\n",
    "slider_row_two = ipw.HBox(\n",
    "    [\n",
    "        node_weight_slider,\n",
    "        structural_weight_slider,\n",
    "        structural_match_bonus_slider,\n",
    "        similarity_threshold_slider,\n",
    "    ],\n",
    "    layout=ipw.Layout(width=\"100%\", justify_content=\"space-between\"),\n",
    ")\n",
    "slider_row_three = ipw.HBox(\n",
    "    [wiki_penalty_slider, arxiv_penalty_slider, num_reads_slider, max_samples_slider],\n",
    "    layout=ipw.Layout(width=\"100%\", justify_content=\"space-between\"),\n",
    ")\n",
    "\n",
    "buttons_panel = ipw.HBox(\n",
    "    [build_graphs_button, run_alignment_button],\n",
    "    layout=ipw.Layout(justify_content=\"flex-start\", gap=\"12px\"),\n",
    ")\n",
    "\n",
    "widget_panel = ipw.VBox(\n",
    "    [\n",
    "        text_input_panel,\n",
    "        ipw.HTML(\"<b>Training and solver controls</b>\"),\n",
    "        slider_row_one,\n",
    "        slider_row_two,\n",
    "        slider_row_three,\n",
    "        buttons_panel,\n",
    "    ],\n",
    "    layout=ipw.Layout(width=\"100%\", gap=\"10px\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f868b",
   "metadata": {},
   "source": [
    "## 3. Bind Events and Display Outputs\n",
    "The final cell wires up the widget callbacks, generates TTL files, trains the joint model, and renders alignment diagnostics so you can see why the QUBO outperforms nearest-neighbor on your toy case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4594e46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d031f83c703444978d8ff03df973a1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(VBox(children=(HTML(value='<b>Wiki entities</b>'), Textarea(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper logic to bind widgets, serialize graphs, and run alignments\n",
    "output_area = ipw.Output(layout=ipw.Layout(border=\"1px solid #dcdcdc\", padding=\"8px\"))\n",
    "alignment_area = ipw.Output(layout=ipw.Layout(border=\"1px solid #dcdcdc\", padding=\"8px\"))\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"[^a-z0-9]+\")\n",
    "current_state: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "def _slug(label: str) -> str:\n",
    "    cleaned = \"\".join(ch if ch.isalnum() or ch in {\"_\", \"-\"} else \"_\" for ch in label.strip())\n",
    "    parts = [part for part in cleaned.split(\"_\") if part]\n",
    "    return \"_\".join(parts) or \"entity\"\n",
    "\n",
    "\n",
    "def _normalize_relation(label: str) -> str:\n",
    "    return \" \".join(label.lower().split())\n",
    "\n",
    "\n",
    "def _parse_meta_value(raw: Any) -> Any:\n",
    "    if raw is None:\n",
    "        return None\n",
    "    return raw\n",
    "\n",
    "\n",
    "def _attribute_text(label: str, meta_value: Any) -> str:\n",
    "    if isinstance(meta_value, dict):\n",
    "        if \"paper_title\" in meta_value:\n",
    "            return str(meta_value[\"paper_title\"])\n",
    "        if \"name\" in meta_value:\n",
    "            return str(meta_value[\"name\"])\n",
    "        return \" \".join(str(v) for v in meta_value.values()) or label\n",
    "    if isinstance(meta_value, (list, tuple, set)):\n",
    "        return \" \".join(str(v) for v in meta_value) or label\n",
    "    if meta_value is None:\n",
    "        return label\n",
    "    return str(meta_value)\n",
    "\n",
    "\n",
    "def _token_set(text: str) -> set[str]:\n",
    "    tokens = [tok for tok in TOKEN_SPLIT_RE.split(text.lower()) if len(tok) > 1]\n",
    "    return set(tokens)\n",
    "\n",
    "\n",
    "def _jaccard(set_a: set[str], set_b: set[str]) -> float:\n",
    "    if not set_a and not set_b:\n",
    "        return 0.0\n",
    "    intersection = len(set_a & set_b)\n",
    "    if intersection == 0:\n",
    "        return 0.0\n",
    "    union = len(set_a | set_b)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def _label_for_uri(payload: Dict[str, Any], uri: URIRef) -> str:\n",
    "    label = payload[\"uri_to_label\"].get(uri)\n",
    "    if label:\n",
    "        return label\n",
    "    text = str(uri)\n",
    "    if \"#\" in text:\n",
    "        return text.rsplit(\"#\", 1)[-1]\n",
    "    if \"/\" in text:\n",
    "        return text.rsplit(\"/\", 1)[-1]\n",
    "    return text\n",
    "\n",
    "\n",
    "def _ensure_tuple(entry: Any) -> Tuple[str, str, str]:\n",
    "    if isinstance(entry, dict):\n",
    "        subj = entry.get(\"subject\") or entry.get(\"subj\")\n",
    "        rel = entry.get(\"relation\") or entry.get(\"predicate\")\n",
    "        obj = entry.get(\"object\") or entry.get(\"obj\")\n",
    "        values = (subj, rel, obj)\n",
    "    elif isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        values = entry\n",
    "    else:\n",
    "        raise ValueError(\"Triple entries must be 3-item lists or dicts with subject, relation, and object keys.\")\n",
    "    subj, rel, obj = [str(item).strip() for item in values]\n",
    "    if not subj or not rel or not obj:\n",
    "        raise ValueError(\"Triple entries cannot contain empty strings.\")\n",
    "    return subj, rel, obj\n",
    "\n",
    "\n",
    "def _parse_entity_block(raw: Any) -> Tuple[List[str], Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {}\n",
    "    labels: List[str] = []\n",
    "    if isinstance(raw, dict):\n",
    "        for key, value in raw.items():\n",
    "            label = str(key).strip()\n",
    "            if not label:\n",
    "                continue\n",
    "            labels.append(label)\n",
    "            parsed_value = _parse_meta_value(value)\n",
    "            if parsed_value is not None and parsed_value != \"\":\n",
    "                meta[label] = parsed_value\n",
    "    elif isinstance(raw, list):\n",
    "        for item in raw:\n",
    "            label = str(item).strip()\n",
    "            if label:\n",
    "                labels.append(label)\n",
    "    else:\n",
    "        raise ValueError(\"Entities must be provided as a JSON array or object.\")\n",
    "    return labels, meta\n",
    "\n",
    "\n",
    "def _attribute_similarity(\n",
    "    wiki_labels: Sequence[str],\n",
    "    wiki_meta: Dict[str, Any],\n",
    "    arxiv_labels: Sequence[str],\n",
    "    arxiv_meta: Dict[str, Any],\n",
    ") -> torch.Tensor:\n",
    "    matrix = torch.zeros((len(wiki_labels), len(arxiv_labels)), dtype=torch.float32)\n",
    "    wiki_sets = [_token_set(_attribute_text(label, wiki_meta.get(label))) for label in wiki_labels]\n",
    "    arxiv_sets = [_token_set(_attribute_text(label, arxiv_meta.get(label))) for label in arxiv_labels]\n",
    "    for i, w_tokens in enumerate(wiki_sets):\n",
    "        for j, a_tokens in enumerate(arxiv_sets):\n",
    "            matrix[i, j] = _jaccard(w_tokens, a_tokens)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def build_graph_payload(\n",
    "    name: str,\n",
    "    entity_labels: Sequence[str],\n",
    "    triples: Sequence[Tuple[str, str, str]],\n",
    "    *,\n",
    "    base_uri: str,\n",
    "    meta: Dict[str, Any] | None = None,\n",
    "):\n",
    "    meta = meta or {}\n",
    "    graph = Graph()\n",
    "    label_to_uri: Dict[str, URIRef] = {}\n",
    "    uri_to_label: Dict[URIRef, str] = {}\n",
    "    attribute_texts: Dict[str, str] = {}\n",
    "    entity_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for label in entity_labels:\n",
    "        uri = URIRef(base_uri + _slug(label))\n",
    "        label_to_uri[label] = uri\n",
    "        uri_to_label[uri] = label\n",
    "        graph.add((uri, RDF.type, ENTITY_CLASS_URI))\n",
    "        graph.add((uri, LABEL_PREDICATE, Literal(label)))\n",
    "        meta_value = meta.get(label)\n",
    "        attr_text = _attribute_text(label, meta_value)\n",
    "        attribute_texts[label] = attr_text\n",
    "        if attr_text:\n",
    "            graph.add((uri, ATTRIBUTE_PREDICATE, Literal(attr_text)))\n",
    "        display_meta = meta_value\n",
    "        if isinstance(meta_value, (dict, list)):\n",
    "            display_meta = json.dumps(meta_value, ensure_ascii=True)\n",
    "        entity_rows.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"meta\": display_meta,\n",
    "                \"attribute_text\": attr_text,\n",
    "                \"uri\": str(uri),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    edge_rows: List[Dict[str, str]] = []\n",
    "    for subj, rel, obj in triples:\n",
    "        if subj not in label_to_uri:\n",
    "            raise ValueError(f\"{name} triple uses unknown subject '{subj}'.\")\n",
    "        if obj not in label_to_uri:\n",
    "            raise ValueError(f\"{name} triple uses unknown object '{obj}'.\")\n",
    "        subj_uri = label_to_uri[subj]\n",
    "        obj_uri = label_to_uri[obj]\n",
    "        rel_uri = URIRef(REL_BASE + _slug(rel))\n",
    "        graph.add((subj_uri, rel_uri, obj_uri))\n",
    "        edge_rows.append(\n",
    "            {\n",
    "                \"subject\": subj,\n",
    "                \"relation\": rel,\n",
    "                \"object\": obj,\n",
    "                \"relation_uri\": str(rel_uri),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    entity_df = (\n",
    "        pd.DataFrame(entity_rows)\n",
    "        .sort_values(\"label\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    edge_df = (\n",
    "        pd.DataFrame(edge_rows).reset_index(drop=True)\n",
    "        if edge_rows\n",
    "        else pd.DataFrame(columns=[\"subject\", \"relation\", \"object\", \"relation_uri\"])\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"graph\": graph,\n",
    "        \"entity_df\": entity_df,\n",
    "        \"edge_df\": edge_df,\n",
    "        \"label_to_uri\": label_to_uri,\n",
    "        \"uri_to_label\": uri_to_label,\n",
    "        \"triples\": list(triples),\n",
    "        \"meta\": meta,\n",
    "        \"attribute_texts\": attribute_texts,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_inputs() -> Tuple[\n",
    "    List[str],\n",
    "    Dict[str, Any],\n",
    "    List[str],\n",
    "    Dict[str, Any],\n",
    "    List[Tuple[str, str, str]],\n",
    "    List[Tuple[str, str, str]],\n",
    "]:\n",
    "    try:\n",
    "        wiki_entities_raw = json.loads(wiki_entities_editor.value)\n",
    "        arxiv_entities_raw = json.loads(arxiv_entities_editor.value)\n",
    "        wiki_triples_raw = json.loads(wiki_triples_editor.value)\n",
    "        arxiv_triples_raw = json.loads(arxiv_triples_editor.value)\n",
    "    except json.JSONDecodeError as exc:\n",
    "        raise ValueError(f\"JSON decode error: {exc}\") from exc\n",
    "\n",
    "    wiki_entities, wiki_meta = _parse_entity_block(wiki_entities_raw)\n",
    "    arxiv_entities, arxiv_meta = _parse_entity_block(arxiv_entities_raw)\n",
    "\n",
    "    if len(wiki_entities) != EXPECTED_WIKI_COUNT:\n",
    "        raise ValueError(\n",
    "            f\"Wiki entity count must be {EXPECTED_WIKI_COUNT}, found {len(wiki_entities)}.\"\n",
    "        )\n",
    "    if len(arxiv_entities) != EXPECTED_ARXIV_COUNT:\n",
    "        raise ValueError(\n",
    "            f\"Paper entity count must be {EXPECTED_ARXIV_COUNT}, found {len(arxiv_entities)}.\"\n",
    "        )\n",
    "\n",
    "    if not isinstance(wiki_triples_raw, list) or not isinstance(arxiv_triples_raw, list):\n",
    "        raise ValueError(\"Triple sections must be JSON arrays of 3-item entries.\")\n",
    "\n",
    "    wiki_triples = [_ensure_tuple(item) for item in wiki_triples_raw]\n",
    "    arxiv_triples = [_ensure_tuple(item) for item in arxiv_triples_raw]\n",
    "\n",
    "    return wiki_entities, wiki_meta, arxiv_entities, arxiv_meta, wiki_triples, arxiv_triples\n",
    "\n",
    "\n",
    "def prepare_state(verbose: bool = False) -> Dict[str, Any]:\n",
    "    (\n",
    "        wiki_entities,\n",
    "        wiki_meta,\n",
    "        arxiv_entities,\n",
    "        arxiv_meta,\n",
    "        wiki_triples,\n",
    "        arxiv_triples,\n",
    "    ) = parse_inputs()\n",
    "\n",
    "    wiki_payload = build_graph_payload(\n",
    "        \"Algorithms\",\n",
    "        wiki_entities,\n",
    "        wiki_triples,\n",
    "        base_uri=WIKI_BASE,\n",
    "        meta=wiki_meta,\n",
    "    )\n",
    "    arxiv_payload = build_graph_payload(\n",
    "        \"Papers\",\n",
    "        arxiv_entities,\n",
    "        arxiv_triples,\n",
    "        base_uri=ARXIV_BASE,\n",
    "        meta=arxiv_meta,\n",
    "    )\n",
    "\n",
    "    wiki_payload[\"graph\"].serialize(str(DEMO_WIKI_PATH), format=\"turtle\")\n",
    "    arxiv_payload[\"graph\"].serialize(str(DEMO_ARXIV_PATH), format=\"turtle\")\n",
    "\n",
    "    current_state.update(\n",
    "        {\n",
    "            \"wiki\": wiki_payload,\n",
    "            \"arxiv\": arxiv_payload,\n",
    "            \"wiki_entities\": wiki_entities,\n",
    "            \"arxiv_entities\": arxiv_entities,\n",
    "            \"wiki_meta\": wiki_meta,\n",
    "            \"arxiv_meta\": arxiv_meta,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        output_area.clear_output()\n",
    "        with output_area:\n",
    "            print(\"Toy graphs written to:\")\n",
    "            print(f\"  Algorithms -> {DEMO_WIKI_PATH}\")\n",
    "            print(f\"  Papers     -> {DEMO_ARXIV_PATH}\")\n",
    "            display(wiki_payload[\"entity_df\"].assign(graph=\"Algorithms\"))\n",
    "            display(arxiv_payload[\"entity_df\"].assign(graph=\"Papers\"))\n",
    "            if not wiki_payload[\"edge_df\"].empty:\n",
    "                display(wiki_payload[\"edge_df\"].assign(graph=\"Algorithms\"))\n",
    "            if not arxiv_payload[\"edge_df\"].empty:\n",
    "                display(arxiv_payload[\"edge_df\"].assign(graph=\"Papers\"))\n",
    "    return current_state\n",
    "\n",
    "\n",
    "def _edges_with_indices(payload: Dict[str, Any], node_map: Dict[URIRef, int]):\n",
    "    edges = []\n",
    "    for subj, rel, obj in payload[\"triples\"]:\n",
    "        subj_uri = payload[\"label_to_uri\"][subj]\n",
    "        obj_uri = payload[\"label_to_uri\"][obj]\n",
    "        edges.append(\n",
    "            {\n",
    "                \"src\": node_map[subj_uri],\n",
    "                \"dst\": node_map[obj_uri],\n",
    "                \"relation\": rel,\n",
    "                \"label\": _normalize_relation(rel),\n",
    "            }\n",
    "        )\n",
    "    return edges\n",
    "\n",
    "\n",
    "def run_nearest_neighbor(\n",
    "    similarity_tensor: torch.Tensor,\n",
    "    wiki_nodes: Sequence[URIRef],\n",
    "    arxiv_nodes: Sequence[URIRef],\n",
    "    threshold: float | None,\n",
    ") -> List[Alignment]:\n",
    "    num_wiki, num_arxiv = similarity_tensor.shape\n",
    "    candidates: List[Tuple[float, int, int]] = []\n",
    "    for i in range(num_wiki):\n",
    "        for j in range(num_arxiv):\n",
    "            score = float(similarity_tensor[i, j])\n",
    "            if threshold is not None and score < threshold:\n",
    "                continue\n",
    "            candidates.append((score, i, j))\n",
    "    candidates.sort(key=lambda item: item[0], reverse=True)\n",
    "\n",
    "    used_wiki: set[int] = set()\n",
    "    used_arxiv: set[int] = set()\n",
    "    alignments: List[Alignment] = []\n",
    "    for score, i, j in candidates:\n",
    "        if i in used_wiki or j in used_arxiv:\n",
    "            continue\n",
    "        used_wiki.add(i)\n",
    "        used_arxiv.add(j)\n",
    "        alignments.append(\n",
    "            Alignment(\n",
    "                wiki_index=i,\n",
    "                arxiv_index=j,\n",
    "                wiki_uri=wiki_nodes[i],\n",
    "                arxiv_uri=arxiv_nodes[j],\n",
    "                similarity=score,\n",
    "            )\n",
    "        )\n",
    "    return alignments\n",
    "\n",
    "\n",
    "def _pad_and_normalize_features(data: torch.Tensor, target_dim: int) -> torch.Tensor:\n",
    "    if data.size(1) < target_dim:\n",
    "        pad_width = (0, target_dim - data.size(1))\n",
    "        data = F.pad(data, pad_width)\n",
    "    return F.normalize(data, dim=1)\n",
    "\n",
    "\n",
    "def handle_build(_=None):\n",
    "    try:\n",
    "        prepare_state(verbose=True)\n",
    "    except Exception as exc:\n",
    "        output_area.clear_output()\n",
    "        with output_area:\n",
    "            print(f\"Error while building graphs: {exc}\")\n",
    "        alignment_area.clear_output()\n",
    "\n",
    "\n",
    "def handle_alignment(_=None):\n",
    "    try:\n",
    "        state = prepare_state(verbose=False)\n",
    "        wiki_payload = state[\"wiki\"]\n",
    "        arxiv_payload = state[\"arxiv\"]\n",
    "\n",
    "        wiki_data, wiki_map = load_pyg_data_from_ttl(\n",
    "            DEMO_WIKI_PATH,\n",
    "            tokenizer=None,\n",
    "            model=None,\n",
    "            use_scibert_features=False,\n",
    "        )\n",
    "        arxiv_data, arxiv_map = load_pyg_data_from_ttl(\n",
    "            DEMO_ARXIV_PATH,\n",
    "            tokenizer=None,\n",
    "            model=None,\n",
    "            use_scibert_features=False,\n",
    "        )\n",
    "\n",
    "        target_dim = max(wiki_data.num_node_features, arxiv_data.num_node_features)\n",
    "        wiki_data.x = _pad_and_normalize_features(wiki_data.x, target_dim)\n",
    "        arxiv_data.x = _pad_and_normalize_features(arxiv_data.x, target_dim)\n",
    "\n",
    "        joint_model = train_gaea_joint(\n",
    "            wiki_data,\n",
    "            arxiv_data,\n",
    "            in_channels=target_dim,\n",
    "            hidden_channels=target_dim,\n",
    "            out_channels=target_dim,\n",
    "            epochs=int(epochs_slider.value),\n",
    "            lr=float(learning_rate_slider.value),\n",
    "            mmd_weight=float(mmd_weight_slider.value),\n",
    "            stats_weight=float(stats_weight_slider.value),\n",
    "            max_samples=int(max_samples_slider.value),\n",
    "        )\n",
    "\n",
    "        joint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            wiki_embed = joint_model.encode(wiki_data.x, wiki_data.edge_index)\n",
    "            arxiv_embed = joint_model.encode(arxiv_data.x, arxiv_data.edge_index)\n",
    "\n",
    "        wiki_embed = F.normalize(wiki_embed.cpu(), dim=1)\n",
    "        arxiv_embed = F.normalize(arxiv_embed.cpu(), dim=1)\n",
    "\n",
    "        embedding_similarity = torch.matmul(wiki_embed, arxiv_embed.T)\n",
    "\n",
    "        wiki_nodes: List[URIRef] = [None] * len(wiki_map)\n",
    "        for uri, idx in wiki_map.items():\n",
    "            wiki_nodes[idx] = uri\n",
    "        arxiv_nodes: List[URIRef] = [None] * len(arxiv_map)\n",
    "        for uri, idx in arxiv_map.items():\n",
    "            arxiv_nodes[idx] = uri\n",
    "\n",
    "        wiki_labels = [_label_for_uri(wiki_payload, uri) for uri in wiki_nodes]\n",
    "        arxiv_labels = [_label_for_uri(arxiv_payload, uri) for uri in arxiv_nodes]\n",
    "\n",
    "        attribute_similarity = _attribute_similarity(\n",
    "            wiki_labels,\n",
    "            state[\"wiki_meta\"],\n",
    "            arxiv_labels,\n",
    "            state[\"arxiv_meta\"],\n",
    "        )\n",
    "        similarity = attribute_similarity\n",
    "\n",
    "        wiki_lookup = {uri: idx for idx, uri in enumerate(wiki_nodes)}\n",
    "        arxiv_lookup = {uri: idx for idx, uri in enumerate(arxiv_nodes)}\n",
    "\n",
    "        wiki_edges = _edges_with_indices(wiki_payload, wiki_map)\n",
    "        arxiv_edges = _edges_with_indices(arxiv_payload, arxiv_map)\n",
    "\n",
    "        structural_weights: Dict[Tuple[int, int, int, int], float] = {}\n",
    "        structural_pairs: List[Dict[str, Any]] = []\n",
    "        for w_edge in wiki_edges:\n",
    "            for a_edge in arxiv_edges:\n",
    "                if w_edge[\"label\"] != a_edge[\"label\"]:\n",
    "                    continue\n",
    "                weight = float(structural_match_bonus_slider.value)\n",
    "                structural_weights[(w_edge[\"src\"], w_edge[\"dst\"], a_edge[\"src\"], a_edge[\"dst\"])] = weight\n",
    "                structural_pairs.append(\n",
    "                    {\n",
    "                        \"relation\": w_edge[\"relation\"],\n",
    "                        \"wiki_edge\": f\"{wiki_labels[w_edge['src']]} → {wiki_labels[w_edge['dst']]}\",\n",
    "                        \"paper_edge\": f\"{arxiv_labels[a_edge['src']]} → {arxiv_labels[a_edge['dst']]}\",\n",
    "                        \"weight\": weight,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        structural_info = {\n",
    "            \"weights\": structural_weights,\n",
    "            \"wiki_edges\": wiki_edges,\n",
    "            \"arxiv_edges\": arxiv_edges,\n",
    "        }\n",
    "\n",
    "        node_info = {\n",
    "            \"wiki_nodes\": wiki_nodes,\n",
    "            \"arxiv_nodes\": arxiv_nodes,\n",
    "            \"wiki_lookup\": wiki_lookup,\n",
    "            \"arxiv_lookup\": arxiv_lookup,\n",
    "            \"similarity\": similarity,\n",
    "            \"wiki_labels\": wiki_labels,\n",
    "            \"arxiv_labels\": arxiv_labels,\n",
    "        }\n",
    "\n",
    "        threshold_value = float(similarity_threshold_slider.value)\n",
    "        threshold = threshold_value if threshold_value > 0 else None\n",
    "\n",
    "        qubo_result = formulate.formulate(\n",
    "            node_info,\n",
    "            structural_info,\n",
    "            node_weight=float(node_weight_slider.value),\n",
    "            structural_weight=float(structural_weight_slider.value),\n",
    "            wiki_penalty=float(wiki_penalty_slider.value),\n",
    "            arxiv_penalty=float(arxiv_penalty_slider.value),\n",
    "            similarity_threshold=threshold,\n",
    "        )\n",
    "\n",
    "        sampleset = _solve_with_simulated_annealing(\n",
    "            qubo_result[\"Q\"],\n",
    "            num_reads=int(num_reads_slider.value),\n",
    "            beta_range=None,\n",
    "            seed=None,\n",
    "            sampler=None,\n",
    "        )\n",
    "        record = sampleset.first\n",
    "        qubo_alignments = _extract_alignments(record.sample, qubo_result[\"reverse_index\"], node_info)\n",
    "        qubo_energy = float(record.energy)\n",
    "\n",
    "        nn_alignments = run_nearest_neighbor(similarity, wiki_nodes, arxiv_nodes, threshold)\n",
    "\n",
    "        def _alignments_to_frame(items: List[Alignment], method: str) -> pd.DataFrame:\n",
    "            rows = []\n",
    "            for item in items:\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"method\": method,\n",
    "                        \"wiki_entity\": wiki_labels[item.wiki_index],\n",
    "                        \"paper_entity\": arxiv_labels[item.arxiv_index],\n",
    "                        \"similarity\": float(item.similarity),\n",
    "                    }\n",
    "                )\n",
    "            return pd.DataFrame(rows)\n",
    "\n",
    "        qubo_df = _alignments_to_frame(qubo_alignments, \"QUBO\")\n",
    "        nn_df = _alignments_to_frame(nn_alignments, \"Nearest neighbor\")\n",
    "        combined_df = (\n",
    "            pd.concat([qubo_df, nn_df], ignore_index=True)\n",
    "            if not qubo_df.empty or not nn_df.empty\n",
    "            else pd.DataFrame(columns=[\"method\", \"wiki_entity\", \"paper_entity\", \"similarity\"])\n",
    "        )\n",
    "\n",
    "        summary_rows = []\n",
    "        for label, items, energy in [\n",
    "            (\"QUBO\", qubo_alignments, qubo_energy),\n",
    "            (\"Nearest neighbor\", nn_alignments, float(\"nan\")),\n",
    "        ]:\n",
    "            sims = [float(item.similarity) for item in items]\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"method\": label,\n",
    "                    \"matches\": len(items),\n",
    "                    \"mean_similarity\": float(np.mean(sims)) if sims else np.nan,\n",
    "                    \"min_similarity\": float(np.min(sims)) if sims else np.nan,\n",
    "                    \"max_similarity\": float(np.max(sims)) if sims else np.nan,\n",
    "                    \"energy\": energy,\n",
    "                }\n",
    "            )\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "        similarity_df = pd.DataFrame(similarity.numpy(), index=wiki_labels, columns=arxiv_labels)\n",
    "        structural_df = pd.DataFrame(structural_pairs)\n",
    "        embedding_df = pd.DataFrame(embedding_similarity.numpy(), index=wiki_labels, columns=arxiv_labels)\n",
    "\n",
    "        alignment_area.clear_output()\n",
    "        with alignment_area:\n",
    "            print(\"Joint GAEA training complete (embeddings shown for reference).\")\n",
    "            print(f\"QUBO best energy: {qubo_energy:.4f}\")\n",
    "            display(summary_df)\n",
    "            if not combined_df.empty:\n",
    "                display(\n",
    "                    combined_df\n",
    "                    .sort_values([\"method\", \"similarity\"], ascending=[True, False])\n",
    "                    .reset_index(drop=True)\n",
    "                )\n",
    "            print(\"Attribute-driven similarity matrix (Jaccard overlaps of text features):\")\n",
    "            display(similarity_df)\n",
    "            print(\"Embedding cosine similarity (optional diagnostic):\")\n",
    "            display(embedding_df)\n",
    "            if not structural_df.empty:\n",
    "                print(\"Structural matches contributing to H_structure:\")\n",
    "                display(structural_df)\n",
    "            else:\n",
    "                print(\"No structural matches discovered; adjust relation labels if needed.\")\n",
    "    except Exception as exc:\n",
    "        alignment_area.clear_output()\n",
    "        with alignment_area:\n",
    "            print(f\"Alignment run failed: {exc}\")\n",
    "\n",
    "\n",
    "build_graphs_button.on_click(handle_build)\n",
    "run_alignment_button.on_click(handle_alignment)\n",
    "\n",
    "ui_container = ipw.VBox(\n",
    "    [\n",
    "        widget_panel,\n",
    "        ipw.HTML(\"<b>Graph summary</b>\"),\n",
    "        output_area,\n",
    "        ipw.HTML(\"<b>Alignment results</b>\"),\n",
    "        alignment_area,\n",
    "    ],\n",
    "    layout=ipw.Layout(width=\"100%\", gap=\"12px\"),\n",
    ")\n",
    "\n",
    "display(ui_container)\n",
    "\n",
    "# Run once to populate default TTL files\n",
    "handle_build()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGA (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
