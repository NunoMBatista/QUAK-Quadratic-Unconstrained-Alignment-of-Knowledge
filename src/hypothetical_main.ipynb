{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3f91fc",
   "metadata": {},
   "source": [
    "# Hypothetical Hybrid GNN-QUBO Alignment Pipeline\n",
    "\n",
    "Keep in mind that this Jupyter notebook's purpose is not to show an advantage of the QUBO method over the NN method. \n",
    "\n",
    "This is just a proposed pipeline for QUBO re-ranking. \n",
    "\n",
    "It implements all of the steps, from data fetching to the QUBO solver.\n",
    "\n",
    "We start by building two regular knowledge graphs (unpruned) and then prune them to simulate how a smaller knowledge graph would look like (in theory, that smaller knowledge graph would contain the ambiguous entities which didn't get a high alignment confidence score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6368166",
   "metadata": {},
   "source": [
    "## Section 1: Load Project Dependencies\n",
    "- Set up the Python path for the project and import every module that the pipeline relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407be95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "import webbrowser\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, IFrame, display\n",
    "\n",
    "repo_root = Path().resolve().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from src.config import *\n",
    "from src.evaluation.solvers import (\n",
    "    solve_alignment_with_annealer,\n",
    "    solve_alignment_with_nearest_neighbor,\n",
    " )\n",
    "from src.kg_construction.fetch_data import fetch_wiki_data, fetch_arxiv_data\n",
    "from src.kg_construction.build_kg import build_unpruned_kgs, prune_kgs\n",
    "from src.embedding.generate_embeddings import (\n",
    "    generate_relation_embeddings,\n",
    "    generate_entity_embeddings,\n",
    " )\n",
    "from src.utils.graph_visualizer import visualize_ttl\n",
    "\n",
    "# [FIX THIS LATER] maintain backward-compatible variable name for existing cells\n",
    "ALIGNED_ENTITIES_CSV = ALIGNED_ENTITIES_ANNEALER_CSV\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# create directories if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WIKI_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARXIV_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _open_in_browser(path, title):\n",
    "    \"\"\"Open a local HTML file in the default browser.\"\"\"\n",
    "    path = Path(path)\n",
    "    webbrowser.open(f\"file://{path.resolve()}\")\n",
    "    display(HTML(f\"<p><i>Opening '{title}' in the browser...</i></p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc7687",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation\n",
    "- Fetch the Wikipedia and arXiv raw data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58de790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching source corpora...\n",
      "-> all requested Wikipedia articles already cached; reusing local files\n",
      "    -> returning 10 Wikipedia summaries.\n",
      "-> all requested arXiv abstracts already cached; reusing local files\n",
      "    -> returning 10 arXiv abstracts.\n",
      "Wikipedia titles: ['Quantum algorithm', 'Post-quantum cryptography', 'Quantum optimization algorithms', 'Quantum computing', \"Shor's algorithm\", \"Grover's algorithm\", 'Noisy intermediate-scale quantum computing', 'Quantum machine learning', 'Quantum counting algorithm', 'Quantum phase estimation algorithm']\n",
      "arXiv IDs: ['2310.03011v2', '2406.13258v3', '2312.13636v3', '0708.0261v1', 'quant-ph/9508027v2', '2108.10854v2', '1801.00862v3', '1611.09347v2', 'quant-ph/9805082v1', 'quant-ph/9511026v1']\n"
     ]
    }
   ],
   "source": [
    "wiki_titles = []\n",
    "arxiv_ids = []\n",
    "\n",
    "\n",
    "print(\"Fetching source corpora...\")\n",
    "wiki_summaries, wiki_titles = fetch_wiki_data()\n",
    "arxiv_abstracts, arxiv_ids = fetch_arxiv_data()\n",
    "print(f\"Wikipedia titles: {wiki_titles}\")\n",
    "print(f\"arXiv IDs: {arxiv_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df661f0",
   "metadata": {},
   "source": [
    "# Section 3: Build the KGs.\n",
    "\n",
    "- Run the NLP pipeline to perform Named Entity Recognition (NER) and Relationship Extraction (RE) on the raw data.\n",
    "\n",
    "- Use the entities and the relations between them to build two large unpruned graphs.\n",
    "\n",
    "- Take the unpruned graphs and reduce them to just a couple entities. In practice, those would be tha \"ambiguous\" entities whose alignment confidence score is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a69849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and pruning knowledge graphs...\n",
      "\n",
      "--- STEP 1: SKIPPING BUILD; UNPRUNED TTLs ARE ALREADY PRESENT ---\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nBuilding and pruning knowledge graphs...\")\n",
    "build_unpruned_kgs(\n",
    "    wiki_data=wiki_summaries, \n",
    "    arxiv_data=arxiv_abstracts\n",
    ")\n",
    "\n",
    "visualize_ttl(KG_WIKI_UNPRUNED_PATH, KG_DIR / \"unpruned_wiki_kg.html\")\n",
    "visualize_ttl(KG_ARXIV_UNPRUNED_PATH, KG_DIR / \"unpruned_arxiv_kg.html\")\n",
    "\n",
    "\n",
    "_open_in_browser(KG_DIR / \"unpruned_wiki_kg.html\", \"Unpruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(KG_DIR / \"unpruned_arxiv_kg.html\", \"Unpruned arXiv Knowledge Graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5f5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 4: PRUNING KGs ---\n",
      "\n",
      "-> pruning Wiki KG...\n",
      "    -> loaded 145 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 14 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "-> pruning arXiv KG...\n",
      "    -> loaded 108 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 10 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prune_kgs()\n",
    "\n",
    "wiki_html = KG_DIR / \"pruned_wiki_kg.html\"\n",
    "arxiv_html = KG_DIR / \"pruned_arxiv_kg.html\"\n",
    "visualize_ttl(KG_WIKI_FINAL_PATH, wiki_html)\n",
    "visualize_ttl(KG_ARXIV_FINAL_PATH, arxiv_html)\n",
    "\n",
    "# Open HTML files in browser\n",
    "_open_in_browser(wiki_html, \"Pruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(arxiv_html, \"Pruned arXiv Knowledge Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886dae0",
   "metadata": {},
   "source": [
    "## Section 4: Generate Embeddings\n",
    "Generate the following embeddings:\n",
    "- Entity embeddings (using a GAE that fine-tunes the SciBERT embeddings)\n",
    "- Relation embeddings (using the SciBERT embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50efc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating relation embeddings...\n",
      "\n",
      "--- Part 1: Generating Relation Embeddings (for H_structure) ---\n",
      "Loading SciBERT model: allenai/scibert_scivocab_cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':1'\n",
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':1'\n",
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':1'\n",
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 175 relation labels.\n",
      "  - aim for\n",
      "  - aims at developing\n",
      "  - also known as\n",
      "  - analyze\n",
      "  - analyzes\n",
      "  - applied to\n",
      "  - applies to\n",
      "  - approximately executes\n",
      "  - based on\n",
      "  - be impacted by\n",
      "  - become more relevant\n",
      "  - believed to be\n",
      "  - can be performed on\n",
      "  - can bruteforce\n",
      "  - can exist in\n",
      "  - can simulate\n",
      "  - can solve faster than\n",
      "  - cannot be efficiently simulated on\n",
      "  - challenges\n",
      "  - characterized by\n",
      "  - class of\n",
      "  - coined by\n",
      "  - coined in year\n",
      "  - combine\n",
      "  - combine into\n",
      "  - combined with\n",
      "  - compare against\n",
      "  - compared to\n",
      "  - considered level\n",
      "  - contains\n",
      "  - contains up to\n",
      "  - could aid in\n",
      "  - could break\n",
      "  - counts solutions for\n",
      "  - date\n",
      "  - defined by\n",
      "  - demonstrate\n",
      "  - demonstrates\n",
      "  - design\n",
      "  - developed by\n",
      "  - devised by\n",
      "  - devised in\n",
      "  - does not require\n",
      "  - enable\n",
      "  - enable solution of\n",
      "  - enables\n",
      "  - estimates\n",
      "  - evaluate against\n",
      "  - evaluate possible\n",
      "  - examines applications of\n",
      "  - executed faster on\n",
      "  - exploits\n",
      "  - explore\n",
      "  - explores\n",
      "  - extends\n",
      "  - face challenge\n",
      "  - follows with\n",
      "  - formulated as\n",
      "  - fuelled by\n",
      "  - gained attention from\n",
      "  - generalized using\n",
      "  - has complexity\n",
      "  - has phase\n",
      "  - have\n",
      "  - have advantage in calculation speed over\n",
      "  - have complexity\n",
      "  - have invested in\n",
      "  - have over\n",
      "  - hinders implementation of\n",
      "  - implement on\n",
      "  - implemented in\n",
      "  - improve space complexity of\n",
      "  - improve time complexity of\n",
      "  - improves efficiency of\n",
      "  - in complexity class\n",
      "  - incapable of\n",
      "  - include\n",
      "  - includes\n",
      "  - incorporate\n",
      "  - independent of\n",
      "  - inefficiently produce\n",
      "  - instance of\n",
      "  - insufficient for\n",
      "  - intersects\n",
      "  - introduced\n",
      "  - introduced by\n",
      "  - introduced in\n",
      "  - involve\n",
      "  - is a\n",
      "  - is algorithm for\n",
      "  - is asymptotically optimal\n",
      "  - is hard on\n",
      "  - is lowest of\n",
      "  - is original motivating application of\n",
      "  - is solution to\n",
      "  - is state of\n",
      "  - lack processing power to break\n",
      "  - lacks\n",
      "  - lacks proposal for\n",
      "  - learns\n",
      "  - may surpass\n",
      "  - measured by\n",
      "  - needed for\n",
      "  - obtainable for\n",
      "  - operates according to\n",
      "  - operates on\n",
      "  - outperform\n",
      "  - outsourced to\n",
      "  - passed\n",
      "  - performs exponentially faster calculations than\n",
      "  - pioneered by\n",
      "  - poses risk to encryption\n",
      "  - prepare for\n",
      "  - presents\n",
      "  - produce\n",
      "  - promising for\n",
      "  - prone to\n",
      "  - proposed algorithm\n",
      "  - proposes\n",
      "  - provide speedup over\n",
      "  - provides\n",
      "  - provides quantum speedup in\n",
      "  - provides speedup\n",
      "  - realizes\n",
      "  - refers to\n",
      "  - relates to\n",
      "  - rely on\n",
      "  - remain\n",
      "  - remains undecidable using\n",
      "  - requires\n",
      "  - requires evaluations\n",
      "  - requires exponential resources for classical simulation\n",
      "  - restricted to domain\n",
      "  - results in\n",
      "  - run on\n",
      "  - runs\n",
      "  - runs faster than\n",
      "  - runs in\n",
      "  - runs on\n",
      "  - samples from\n",
      "  - searches for\n",
      "  - seen as\n",
      "  - sensitive to\n",
      "  - serves as subroutine in\n",
      "  - serves same function as\n",
      "  - should strive for\n",
      "  - shows superpolynomial speedup\n",
      "  - similar to\n",
      "  - solve\n",
      "  - solved by\n",
      "  - solves\n",
      "  - span across\n",
      "  - studies\n",
      "  - subclass of\n",
      "  - suffers from\n",
      "  - thought secure against\n",
      "  - threaten\n",
      "  - transform\n",
      "  - type of\n",
      "  - use\n",
      "  - used as subroutine in\n",
      "  - used for\n",
      "  - used in\n",
      "  - uses\n",
      "  - uses complexity\n",
      "  - uses feature\n",
      "  - uses gate complexity\n",
      "  - uses wave interference\n",
      "  - usually type of\n",
      "  - utilizes\n",
      "  - vulnerable to\n",
      "  - will be available\n",
      "  - will be useful for\n",
      "  - will limit\n",
      "  - will not change\n",
      "Successfully saved relation embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/relation_embeddings.npz\n",
      "\n",
      "Generating entity embeddings...\n",
      "\n",
      "--- Part 2: Generating Entity Embeddings (for H_node) ---\n",
      "\n",
      "Processing Wiki KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 132 nodes, 404 edges.\n",
      "\n",
      "Processing ArXiv KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 110 nodes, 316 edges.\n",
      "[ANCHOR] Using 2 anchor alignments during embedding training.\n",
      "  Training joint GAEA for 500 epochs...\n",
      "    Epoch 1/500, Loss: 1.4557, Wiki recon: 0.7237, ArXiv recon: 0.7240, MMD: 0.0152, Stats: 0.0000, Anchor: 0.0004, Wiki pos/neg: 0.627/0.625, ArXiv pos/neg: 0.627/0.625\n",
      "    Epoch 20/500, Loss: 1.4474, Wiki recon: 0.7227, ArXiv recon: 0.7230, MMD: 0.0023, Stats: 0.0000, Anchor: 0.0005, Wiki pos/neg: 0.628/0.625, ArXiv pos/neg: 0.627/0.625\n",
      "    Epoch 40/500, Loss: 1.4421, Wiki recon: 0.7213, ArXiv recon: 0.7214, MMD: -0.0023, Stats: 0.0000, Anchor: 0.0005, Wiki pos/neg: 0.632/0.626, ArXiv pos/neg: 0.632/0.626\n",
      "    Epoch 60/500, Loss: 1.4279, Wiki recon: 0.7137, ArXiv recon: 0.7148, MMD: -0.0031, Stats: 0.0001, Anchor: 0.0010, Wiki pos/neg: 0.648/0.629, ArXiv pos/neg: 0.647/0.630\n",
      "    Epoch 80/500, Loss: 1.3743, Wiki recon: 0.6862, ArXiv recon: 0.6854, MMD: -0.0057, Stats: 0.0001, Anchor: 0.0055, Wiki pos/neg: 0.680/0.624, ArXiv pos/neg: 0.680/0.624\n",
      "    Epoch 100/500, Loss: 1.3333, Wiki recon: 0.6712, ArXiv recon: 0.6627, MMD: -0.0061, Stats: 0.0002, Anchor: 0.0024, Wiki pos/neg: 0.698/0.617, ArXiv pos/neg: 0.704/0.613\n",
      "    Epoch 120/500, Loss: 1.3456, Wiki recon: 0.6773, ArXiv recon: 0.6674, MMD: -0.0050, Stats: 0.0001, Anchor: 0.0035, Wiki pos/neg: 0.704/0.624, ArXiv pos/neg: 0.711/0.619\n",
      "    Epoch 140/500, Loss: 1.3281, Wiki recon: 0.6717, ArXiv recon: 0.6568, MMD: -0.0056, Stats: 0.0005, Anchor: 0.0024, Wiki pos/neg: 0.707/0.620, ArXiv pos/neg: 0.714/0.612\n",
      "    Epoch 160/500, Loss: 1.3234, Wiki recon: 0.6634, ArXiv recon: 0.6566, MMD: -0.0048, Stats: 0.0003, Anchor: 0.0057, Wiki pos/neg: 0.718/0.621, ArXiv pos/neg: 0.721/0.616\n",
      "    Epoch 180/500, Loss: 1.3382, Wiki recon: 0.6685, ArXiv recon: 0.6688, MMD: -0.0043, Stats: 0.0006, Anchor: 0.0030, Wiki pos/neg: 0.721/0.626, ArXiv pos/neg: 0.721/0.626\n",
      "    Epoch 200/500, Loss: 1.2996, Wiki recon: 0.6566, ArXiv recon: 0.6439, MMD: -0.0053, Stats: 0.0005, Anchor: 0.0018, Wiki pos/neg: 0.720/0.617, ArXiv pos/neg: 0.723/0.608\n",
      "    Epoch 220/500, Loss: 1.3180, Wiki recon: 0.6678, ArXiv recon: 0.6486, MMD: -0.0052, Stats: 0.0006, Anchor: 0.0042, Wiki pos/neg: 0.718/0.624, ArXiv pos/neg: 0.722/0.610\n",
      "    Epoch 240/500, Loss: 1.2740, Wiki recon: 0.6348, ArXiv recon: 0.6403, MMD: -0.0051, Stats: 0.0008, Anchor: 0.0013, Wiki pos/neg: 0.721/0.601, ArXiv pos/neg: 0.724/0.606\n",
      "    Epoch 260/500, Loss: 1.3153, Wiki recon: 0.6611, ArXiv recon: 0.6551, MMD: -0.0050, Stats: 0.0007, Anchor: 0.0015, Wiki pos/neg: 0.722/0.620, ArXiv pos/neg: 0.724/0.616\n",
      "    Epoch 280/500, Loss: 1.2777, Wiki recon: 0.6375, ArXiv recon: 0.6382, MMD: -0.0054, Stats: 0.0006, Anchor: 0.0046, Wiki pos/neg: 0.726/0.603, ArXiv pos/neg: 0.724/0.604\n",
      "    Epoch 300/500, Loss: 1.2935, Wiki recon: 0.6577, ArXiv recon: 0.6360, MMD: -0.0057, Stats: 0.0005, Anchor: 0.0026, Wiki pos/neg: 0.725/0.619, ArXiv pos/neg: 0.725/0.602\n",
      "    Epoch 320/500, Loss: 1.2853, Wiki recon: 0.6359, ArXiv recon: 0.6500, MMD: -0.0044, Stats: 0.0012, Anchor: 0.0015, Wiki pos/neg: 0.725/0.602, ArXiv pos/neg: 0.725/0.613\n",
      "    Epoch 340/500, Loss: 1.3004, Wiki recon: 0.6510, ArXiv recon: 0.6458, MMD: -0.0049, Stats: 0.0009, Anchor: 0.0059, Wiki pos/neg: 0.723/0.611, ArXiv pos/neg: 0.725/0.608\n",
      "    Epoch 360/500, Loss: 1.2638, Wiki recon: 0.6321, ArXiv recon: 0.6294, MMD: -0.0053, Stats: 0.0006, Anchor: 0.0049, Wiki pos/neg: 0.726/0.597, ArXiv pos/neg: 0.727/0.596\n",
      "    Epoch 380/500, Loss: 1.2616, Wiki recon: 0.6236, ArXiv recon: 0.6354, MMD: -0.0042, Stats: 0.0016, Anchor: 0.0045, Wiki pos/neg: 0.722/0.590, ArXiv pos/neg: 0.725/0.599\n",
      "    Epoch 400/500, Loss: 1.2667, Wiki recon: 0.6344, ArXiv recon: 0.6325, MMD: -0.0046, Stats: 0.0014, Anchor: 0.0019, Wiki pos/neg: 0.724/0.598, ArXiv pos/neg: 0.727/0.597\n",
      "    Epoch 420/500, Loss: 1.2730, Wiki recon: 0.6448, ArXiv recon: 0.6279, MMD: -0.0045, Stats: 0.0016, Anchor: 0.0023, Wiki pos/neg: 0.723/0.606, ArXiv pos/neg: 0.725/0.593\n",
      "    Epoch 440/500, Loss: 1.2779, Wiki recon: 0.6430, ArXiv recon: 0.6344, MMD: -0.0054, Stats: 0.0008, Anchor: 0.0031, Wiki pos/neg: 0.726/0.605, ArXiv pos/neg: 0.726/0.598\n",
      "    Epoch 460/500, Loss: 1.2751, Wiki recon: 0.6226, ArXiv recon: 0.6450, MMD: -0.0049, Stats: 0.0014, Anchor: 0.0098, Wiki pos/neg: 0.724/0.588, ArXiv pos/neg: 0.727/0.606\n",
      "    Epoch 480/500, Loss: 1.2350, Wiki recon: 0.6018, ArXiv recon: 0.6331, MMD: -0.0051, Stats: 0.0010, Anchor: 0.0025, Wiki pos/neg: 0.724/0.573, ArXiv pos/neg: 0.725/0.597\n",
      "    Epoch 500/500, Loss: 1.2260, Wiki recon: 0.6146, ArXiv recon: 0.6084, MMD: -0.0051, Stats: 0.0014, Anchor: 0.0054, Wiki pos/neg: 0.724/0.583, ArXiv pos/neg: 0.726/0.579\n",
      "Saved Wiki entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_wiki.pt\n",
      "Saved ArXiv entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_arxiv.pt\n"
     ]
    }
   ],
   "source": [
    "RUN_EMBEDDINGS = True\n",
    "\n",
    "if RUN_EMBEDDINGS:\n",
    "    print(\"Generating relation embeddings...\")\n",
    "    generate_relation_embeddings()\n",
    "    print(\"\\nGenerating entity embeddings...\")\n",
    "    generate_entity_embeddings()\n",
    "else:\n",
    "    print(\"Skipping embedding generation; using cached tensors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61f0a8",
   "metadata": {},
   "source": [
    "## Section 5: Formulate the problem as a QUBO and solve it\n",
    "Perform the QUBO formulation:\n",
    "$$\n",
    "H_{total} = \\underbrace{\\sum_{i,a}{-S(i, a) \\cdot x_{i,a}}}_{H_{\\text{node}}} + \\underbrace{\\sum_{i,j,a,b}{-w_{ij,ab} \\cdot x_{i,a} \\cdot x_{j,b}}}_{H_{\\text{structure}}} + \\underbrace{\\sum_{i} P_{1} \\sum_{a=1}^M \\sum_{b=a+1}^M x_{i,a} x_{i,b}}_{\\text{Constraint}\\ 1} + \\underbrace{\\sum_{a} P_{2} \\sum_{i=1}^N \\sum_{j=i+1}^N x_{i,a} x_{j,a}}_{\\text{Constraint}\\ 2}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $x_{i,a}$: A binary variable (1 or 0) that is 1 if we align entity $i$ from KG1 with entity $a$ from KG2.\n",
    "  - $S(i,a)$: The similarity score between entity $i$ and $a$, derived from the GAE embeddings.\n",
    "  - $w_{ij,ab}$: The structural similarity weight, derived from the SciBERT relation embeddings.\n",
    "  - $P_1, P_2$: Large positive penalty constants to enforce the constraints.\n",
    "  - Constraint 1: Enforces that each entity $i$ in KG1 maps to at most one entity in KG2. If $i$ matches zero entities, the penalty is 0. If it matches one, the penalty is 0. If it matches two or more, the penalty is high.\n",
    "  - Constraint 2: Enforces that each entity $a$ in KG2 is mapped to by at most one entity from KG1. This allows entities to remain unaligned, making the formulation more robust to realistic KGs that do not have perfect 1-to-1 overlap.\n",
    "\n",
    "And solve it using quantum annealing (more details in the README.md file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aab8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the alignment QUBO...\n",
      "[QUBO] candidate variables: 18, structural pairs: 5\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix.csv with dimension 18×18\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_node.csv with dimension 18×18\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_structure.csv with dimension 18×18\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_penalty.csv with dimension 18×18\n",
      "[QUBO] running simulated annealer with num_reads=100, beta_range=None, seed=None\n",
      "[QUBO] best sample energy=-6.2605 produced 7 alignments\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/aligned_kg.html\n",
      "[QUBO] alignment report saved to /home/nuno/Documents/QUBO-KGA/output/alignments/alignment_annealer.csv with 7 matches and 4 unaligned entries\n"
     ]
    }
   ],
   "source": [
    "SOLVE_QUBO = True\n",
    "\n",
    "result = None\n",
    "if SOLVE_QUBO:\n",
    "    print(\"\\nSolving the alignment QUBO...\")\n",
    "    result = solve_alignment_with_annealer(\n",
    "        similarity_threshold=DEFAULT_SIMILARITY_THRESHOLD,\n",
    "        max_structural_pairs=2000,\n",
    "        visualize=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping QUBO solve; falling back to existing artefacts.\")\n",
    "\n",
    "if result is None:\n",
    "    result = SimpleNamespace(\n",
    "        alignments=[],\n",
    "        energy=float(\"nan\"),\n",
    "        sampleset=None,\n",
    "        aligned_graph_path=KG_ALIGNED_PATH,\n",
    "        aligned_graph_html=(KG_DIR / \"kg_aligned.html\"),\n",
    "        alignment_report_path=ALIGNED_ENTITIES_ANNEALER_CSV,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e3d33",
   "metadata": {},
   "source": [
    "## Section 6: Display HTML Visualizations\n",
    "Render the pruned graphs, aligned knowledge graph, and alignment report directly within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7824dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Aligned Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_entity</th>\n",
       "      <th>arxiv_entity</th>\n",
       "      <th>not_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qubit</td>\n",
       "      <td>qubits</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Shor</td>\n",
       "      <td>Shor's quantum algorithms</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shor's algorithm</td>\n",
       "      <td>Grover algorithm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quantum computer technology</td>\n",
       "      <td>quantum computing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post-quantum cryptography</td>\n",
       "      <td>post-quantum cryptography</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Noisy intermediate-scale quantum NISQ computing</td>\n",
       "      <td>quantum machine learning</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quantum optimization algorithms</td>\n",
       "      <td>NISQ devices</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wiki: Grover's algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wiki: quantum machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv: Deutsch's algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv: QOA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        wiki_entity  \\\n",
       "0                                             qubit   \n",
       "1                                        Peter Shor   \n",
       "2                                  Shor's algorithm   \n",
       "3                       quantum computer technology   \n",
       "4                         Post-quantum cryptography   \n",
       "5   Noisy intermediate-scale quantum NISQ computing   \n",
       "6                   Quantum optimization algorithms   \n",
       "7                                               NaN   \n",
       "8                                               NaN   \n",
       "9                                               NaN   \n",
       "10                                              NaN   \n",
       "\n",
       "                 arxiv_entity                     not_aligned  \n",
       "0                      qubits                             NaN  \n",
       "1   Shor's quantum algorithms                             NaN  \n",
       "2            Grover algorithm                             NaN  \n",
       "3           quantum computing                             NaN  \n",
       "4   post-quantum cryptography                             NaN  \n",
       "5    quantum machine learning                             NaN  \n",
       "6                NISQ devices                             NaN  \n",
       "7                         NaN        wiki: Grover's algorithm  \n",
       "8                         NaN  wiki: quantum machine learning  \n",
       "9                         NaN      arxiv: Deutsch's algorithm  \n",
       "10                        NaN                      arxiv: QOA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':1'\n"
     ]
    }
   ],
   "source": [
    "aligned_html = KG_DIR / \"kg_aligned.html\"\n",
    "# create the visualizations\n",
    "\n",
    "visualize_ttl(KG_ALIGNED_PATH, aligned_html)\n",
    "\n",
    "_open_in_browser(aligned_html, \"Aligned Knowledge Graph\")\n",
    "\n",
    "# # Display only the DataFrame in the notebook\n",
    "display(pd.read_csv(ALIGNED_ENTITIES_ANNEALER_CSV))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
