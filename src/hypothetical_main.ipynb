{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3f91fc",
   "metadata": {},
   "source": [
    "# Hypothetical Hybrid GNN-QUBO Alignment Pipeline\n",
    "\n",
    "Keep in mind that this Jupyter notebook's purpose is not to show an advantage of the QUBO method over the NN method. \n",
    "\n",
    "This is just a proposed pipeline for QUBO re-ranking. \n",
    "\n",
    "It implements all of the steps, from data fetching to the QUBO solver.\n",
    "\n",
    "We start by building two regular knowledge graphs (unpruned) and then prune them to simulate how a smaller knowledge graph would look like (in theory, that smaller knowledge graph would contain the ambiguous entities which didn't get a high alignment confidence score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6368166",
   "metadata": {},
   "source": [
    "## Section 1: Load Project Dependencies\n",
    "- Set up the Python path for the project and import every module that the pipeline relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407be95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuno/python_envs/KGA/lib/python3.11/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/nuno/python_envs/KGA/lib/python3.11/site-packages/torch/cuda/__init__.py:1034: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/nuno/python_envs/KGA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nuno/python_envs/KGA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "import webbrowser\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, IFrame, display\n",
    "\n",
    "repo_root = Path().resolve().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from src.config import *\n",
    "from src.evaluation.solvers import (\n",
    "    solve_alignment_with_annealer,\n",
    "    solve_alignment_with_nearest_neighbor,\n",
    " )\n",
    "from src.kg_construction.fetch_data import fetch_wiki_data, fetch_arxiv_data\n",
    "from src.kg_construction.build_kg import build_unpruned_kgs, prune_kgs\n",
    "from src.embedding.generate_embeddings import (\n",
    "    generate_relation_embeddings,\n",
    "    generate_entity_embeddings,\n",
    " )\n",
    "from src.utils.graph_visualizer import visualize_ttl\n",
    "\n",
    "# [FIX THIS LATER] maintain backward-compatible variable name for existing cells\n",
    "ALIGNED_ENTITIES_CSV = ALIGNED_ENTITIES_ANNEALER_CSV\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# create directories if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WIKI_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARXIV_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _open_in_browser(path, title):\n",
    "    \"\"\"Open a local HTML file in the default browser.\"\"\"\n",
    "    path = Path(path)\n",
    "    webbrowser.open(f\"file://{path.resolve()}\")\n",
    "    display(HTML(f\"<p><i>Opening '{title}' in the browser...</i></p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc7687",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation\n",
    "- Fetch the Wikipedia and arXiv raw data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58de790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching source corpora...\n",
      "-> all requested Wikipedia articles already cached; reusing local files\n",
      "    -> returning 10 Wikipedia summaries.\n",
      "-> all requested arXiv abstracts already cached; reusing local files\n",
      "    -> returning 10 arXiv abstracts.\n",
      "Wikipedia titles: ['Quantum algorithm', 'Post-quantum cryptography', 'Quantum optimization algorithms', 'Quantum computing', \"Shor's algorithm\", \"Grover's algorithm\", 'Noisy intermediate-scale quantum computing', 'Quantum machine learning', 'Quantum counting algorithm', 'Quantum phase estimation algorithm']\n",
      "arXiv IDs: ['2310.03011v2', '2406.13258v3', '2312.13636v3', '0708.0261v1', 'quant-ph/9508027v2', '2108.10854v2', '1801.00862v3', '1611.09347v2', 'quant-ph/9805082v1', 'quant-ph/9511026v1']\n"
     ]
    }
   ],
   "source": [
    "wiki_titles = []\n",
    "arxiv_ids = []\n",
    "\n",
    "\n",
    "print(\"Fetching source corpora...\")\n",
    "wiki_summaries, wiki_titles = fetch_wiki_data()\n",
    "arxiv_abstracts, arxiv_ids = fetch_arxiv_data()\n",
    "print(f\"Wikipedia titles: {wiki_titles}\")\n",
    "print(f\"arXiv IDs: {arxiv_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df661f0",
   "metadata": {},
   "source": [
    "# Section 3: Build the KGs.\n",
    "\n",
    "- Run the NLP pipeline to perform Named Entity Recognition (NER) and Relationship Extraction (RE) on the raw data.\n",
    "\n",
    "- Use the entities and the relations between them to build two large unpruned graphs.\n",
    "\n",
    "- Take the unpruned graphs and reduce them to just a couple entities. In practice, those would be tha \"ambiguous\" entities whose alignment confidence score is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a69849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and pruning knowledge graphs...\n",
      "\n",
      "--- STEP 1: SKIPPING BUILD; UNPRUNED TTLs ARE ALREADY PRESENT ---\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nBuilding and pruning knowledge graphs...\")\n",
    "build_unpruned_kgs(\n",
    "    wiki_data=wiki_summaries, \n",
    "    arxiv_data=arxiv_abstracts\n",
    ")\n",
    "\n",
    "visualize_ttl(KG_WIKI_UNPRUNED_PATH, KG_DIR / \"unpruned_wiki_kg.html\")\n",
    "visualize_ttl(KG_ARXIV_UNPRUNED_PATH, KG_DIR / \"unpruned_arxiv_kg.html\")\n",
    "\n",
    "\n",
    "_open_in_browser(KG_DIR / \"unpruned_wiki_kg.html\", \"Unpruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(KG_DIR / \"unpruned_arxiv_kg.html\", \"Unpruned arXiv Knowledge Graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5f5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 4: PRUNING KGs ---\n",
      "\n",
      "-> pruning Wiki KG...\n",
      "    -> loaded 145 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 14 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "-> pruning arXiv KG...\n",
      "    -> loaded 108 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 10 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prune_kgs()\n",
    "\n",
    "wiki_html = KG_DIR / \"pruned_wiki_kg.html\"\n",
    "arxiv_html = KG_DIR / \"pruned_arxiv_kg.html\"\n",
    "visualize_ttl(KG_WIKI_FINAL_PATH, wiki_html)\n",
    "visualize_ttl(KG_ARXIV_FINAL_PATH, arxiv_html)\n",
    "\n",
    "# Open HTML files in browser\n",
    "_open_in_browser(wiki_html, \"Pruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(arxiv_html, \"Pruned arXiv Knowledge Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886dae0",
   "metadata": {},
   "source": [
    "## Section 4: Generate Embeddings\n",
    "Generate the following embeddings:\n",
    "- Entity embeddings (using a GAE that fine-tunes the SciBERT embeddings)\n",
    "- Relation embeddings (using the SciBERT embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50efc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating relation embeddings...\n",
      "\n",
      "--- Part 1: Generating Relation Embeddings (for H_structure) ---\n",
      "Loading SciBERT model: allenai/scibert_scivocab_cased...\n",
      "Discovered 175 relation labels.\n",
      "  - aim for\n",
      "  - aims at developing\n",
      "  - also known as\n",
      "  - analyze\n",
      "  - analyzes\n",
      "  - applied to\n",
      "  - applies to\n",
      "  - approximately executes\n",
      "Discovered 175 relation labels.\n",
      "  - aim for\n",
      "  - aims at developing\n",
      "  - also known as\n",
      "  - analyze\n",
      "  - analyzes\n",
      "  - applied to\n",
      "  - applies to\n",
      "  - approximately executes\n",
      "  - based on\n",
      "  - be impacted by\n",
      "  - become more relevant\n",
      "  - believed to be\n",
      "  - can be performed on\n",
      "  - can bruteforce\n",
      "  - can exist in\n",
      "  - can simulate\n",
      "  - based on\n",
      "  - be impacted by\n",
      "  - become more relevant\n",
      "  - believed to be\n",
      "  - can be performed on\n",
      "  - can bruteforce\n",
      "  - can exist in\n",
      "  - can simulate\n",
      "  - can solve faster than\n",
      "  - cannot be efficiently simulated on\n",
      "  - challenges\n",
      "  - characterized by\n",
      "  - class of\n",
      "  - coined by\n",
      "  - coined in year\n",
      "  - combine\n",
      "  - can solve faster than\n",
      "  - cannot be efficiently simulated on\n",
      "  - challenges\n",
      "  - characterized by\n",
      "  - class of\n",
      "  - coined by\n",
      "  - coined in year\n",
      "  - combine\n",
      "  - combine into\n",
      "  - combined with\n",
      "  - compare against\n",
      "  - compared to\n",
      "  - considered level\n",
      "  - contains\n",
      "  - contains up to\n",
      "  - could aid in\n",
      "  - could break\n",
      "  - combine into\n",
      "  - combined with\n",
      "  - compare against\n",
      "  - compared to\n",
      "  - considered level\n",
      "  - contains\n",
      "  - contains up to\n",
      "  - could aid in\n",
      "  - could break\n",
      "  - counts solutions for\n",
      "  - date\n",
      "  - defined by\n",
      "  - demonstrate\n",
      "  - demonstrates\n",
      "  - design\n",
      "  - developed by\n",
      "  - devised by\n",
      "  - counts solutions for\n",
      "  - date\n",
      "  - defined by\n",
      "  - demonstrate\n",
      "  - demonstrates\n",
      "  - design\n",
      "  - developed by\n",
      "  - devised by\n",
      "  - devised in\n",
      "  - does not require\n",
      "  - enable\n",
      "  - enable solution of\n",
      "  - enables\n",
      "  - estimates\n",
      "  - evaluate against\n",
      "  - devised in\n",
      "  - does not require\n",
      "  - enable\n",
      "  - enable solution of\n",
      "  - enables\n",
      "  - estimates\n",
      "  - evaluate against\n",
      "  - evaluate possible\n",
      "  - examines applications of\n",
      "  - executed faster on\n",
      "  - exploits\n",
      "  - explore\n",
      "  - explores\n",
      "  - extends\n",
      "  - face challenge\n",
      "  - evaluate possible\n",
      "  - examines applications of\n",
      "  - executed faster on\n",
      "  - exploits\n",
      "  - explore\n",
      "  - explores\n",
      "  - extends\n",
      "  - face challenge\n",
      "  - follows with\n",
      "  - formulated as\n",
      "  - fuelled by\n",
      "  - gained attention from\n",
      "  - generalized using\n",
      "  - has complexity\n",
      "  - has phase\n",
      "  - have\n",
      "  - have advantage in calculation speed over\n",
      "  - follows with\n",
      "  - formulated as\n",
      "  - fuelled by\n",
      "  - gained attention from\n",
      "  - generalized using\n",
      "  - has complexity\n",
      "  - has phase\n",
      "  - have\n",
      "  - have advantage in calculation speed over\n",
      "  - have complexity\n",
      "  - have invested in\n",
      "  - have over\n",
      "  - hinders implementation of\n",
      "  - implement on\n",
      "  - implemented in\n",
      "  - improve space complexity of\n",
      "  - improve time complexity of\n",
      "  - have complexity\n",
      "  - have invested in\n",
      "  - have over\n",
      "  - hinders implementation of\n",
      "  - implement on\n",
      "  - implemented in\n",
      "  - improve space complexity of\n",
      "  - improve time complexity of\n",
      "  - improves efficiency of\n",
      "  - in complexity class\n",
      "  - incapable of\n",
      "  - include\n",
      "  - includes\n",
      "  - incorporate\n",
      "  - independent of\n",
      "  - inefficiently produce\n",
      "  - improves efficiency of\n",
      "  - in complexity class\n",
      "  - incapable of\n",
      "  - include\n",
      "  - includes\n",
      "  - incorporate\n",
      "  - independent of\n",
      "  - inefficiently produce\n",
      "  - instance of\n",
      "  - insufficient for\n",
      "  - intersects\n",
      "  - introduced\n",
      "  - introduced by\n",
      "  - introduced in\n",
      "  - involve\n",
      "  - is a\n",
      "  - instance of\n",
      "  - insufficient for\n",
      "  - intersects\n",
      "  - introduced\n",
      "  - introduced by\n",
      "  - introduced in\n",
      "  - involve\n",
      "  - is a\n",
      "  - is algorithm for\n",
      "  - is asymptotically optimal\n",
      "  - is hard on\n",
      "  - is lowest of\n",
      "  - is original motivating application of\n",
      "  - is solution to\n",
      "  - is state of\n",
      "  - lack processing power to break\n",
      "  - lacks\n",
      "  - is algorithm for\n",
      "  - is asymptotically optimal\n",
      "  - is hard on\n",
      "  - is lowest of\n",
      "  - is original motivating application of\n",
      "  - is solution to\n",
      "  - is state of\n",
      "  - lack processing power to break\n",
      "  - lacks\n",
      "  - lacks proposal for\n",
      "  - learns\n",
      "  - may surpass\n",
      "  - measured by\n",
      "  - needed for\n",
      "  - obtainable for\n",
      "  - operates according to\n",
      "  - operates on\n",
      "  - outperform\n",
      "  - lacks proposal for\n",
      "  - learns\n",
      "  - may surpass\n",
      "  - measured by\n",
      "  - needed for\n",
      "  - obtainable for\n",
      "  - operates according to\n",
      "  - operates on\n",
      "  - outperform\n",
      "  - outsourced to\n",
      "  - passed\n",
      "  - performs exponentially faster calculations than\n",
      "  - pioneered by\n",
      "  - poses risk to encryption\n",
      "  - prepare for\n",
      "  - presents\n",
      "  - produce\n",
      "  - promising for\n",
      "  - outsourced to\n",
      "  - passed\n",
      "  - performs exponentially faster calculations than\n",
      "  - pioneered by\n",
      "  - poses risk to encryption\n",
      "  - prepare for\n",
      "  - presents\n",
      "  - produce\n",
      "  - promising for\n",
      "  - prone to\n",
      "  - proposed algorithm\n",
      "  - proposes\n",
      "  - provide speedup over\n",
      "  - provides\n",
      "  - provides quantum speedup in\n",
      "  - provides speedup\n",
      "  - realizes\n",
      "  - refers to\n",
      "  - prone to\n",
      "  - proposed algorithm\n",
      "  - proposes\n",
      "  - provide speedup over\n",
      "  - provides\n",
      "  - provides quantum speedup in\n",
      "  - provides speedup\n",
      "  - realizes\n",
      "  - refers to\n",
      "  - relates to\n",
      "  - rely on\n",
      "  - remain\n",
      "  - remains undecidable using\n",
      "  - requires\n",
      "  - requires evaluations\n",
      "  - requires exponential resources for classical simulation\n",
      "  - restricted to domain\n",
      "  - results in\n",
      "  - relates to\n",
      "  - rely on\n",
      "  - remain\n",
      "  - remains undecidable using\n",
      "  - requires\n",
      "  - requires evaluations\n",
      "  - requires exponential resources for classical simulation\n",
      "  - restricted to domain\n",
      "  - results in\n",
      "  - run on\n",
      "  - runs\n",
      "  - runs faster than\n",
      "  - runs in\n",
      "  - runs on\n",
      "  - samples from\n",
      "  - searches for\n",
      "  - seen as\n",
      "  - run on\n",
      "  - runs\n",
      "  - runs faster than\n",
      "  - runs in\n",
      "  - runs on\n",
      "  - samples from\n",
      "  - searches for\n",
      "  - seen as\n",
      "  - sensitive to\n",
      "  - serves as subroutine in\n",
      "  - serves same function as\n",
      "  - should strive for\n",
      "  - shows superpolynomial speedup\n",
      "  - similar to\n",
      "  - solve\n",
      "  - solved by\n",
      "  - sensitive to\n",
      "  - serves as subroutine in\n",
      "  - serves same function as\n",
      "  - should strive for\n",
      "  - shows superpolynomial speedup\n",
      "  - similar to\n",
      "  - solve\n",
      "  - solved by\n",
      "  - solves\n",
      "  - span across\n",
      "  - studies\n",
      "  - subclass of\n",
      "  - suffers from\n",
      "  - thought secure against\n",
      "  - threaten\n",
      "  - transform\n",
      "  - type of\n",
      "  - solves\n",
      "  - span across\n",
      "  - studies\n",
      "  - subclass of\n",
      "  - suffers from\n",
      "  - thought secure against\n",
      "  - threaten\n",
      "  - transform\n",
      "  - type of\n",
      "  - use\n",
      "  - used as subroutine in\n",
      "  - used for\n",
      "  - used in\n",
      "  - uses\n",
      "  - uses complexity\n",
      "  - uses feature\n",
      "  - uses gate complexity\n",
      "  - use\n",
      "  - used as subroutine in\n",
      "  - used for\n",
      "  - used in\n",
      "  - uses\n",
      "  - uses complexity\n",
      "  - uses feature\n",
      "  - uses gate complexity\n",
      "  - uses wave interference\n",
      "  - usually type of\n",
      "  - utilizes\n",
      "  - vulnerable to\n",
      "  - will be available\n",
      "  - will be useful for\n",
      "  - will limit\n",
      "  - will not change\n",
      "  - uses wave interference\n",
      "  - usually type of\n",
      "  - utilizes\n",
      "  - vulnerable to\n",
      "  - will be available\n",
      "  - will be useful for\n",
      "  - will limit\n",
      "  - will not change\n",
      "Successfully saved relation embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/relation_embeddings.npz\n",
      "\n",
      "Generating entity embeddings...\n",
      "\n",
      "--- Part 2: Generating Entity Embeddings (for H_node) ---\n",
      "Successfully saved relation embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/relation_embeddings.npz\n",
      "\n",
      "Generating entity embeddings...\n",
      "\n",
      "--- Part 2: Generating Entity Embeddings (for H_node) ---\n",
      "\n",
      "Processing Wiki KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "\n",
      "Processing Wiki KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 132 nodes, 404 edges.\n",
      "\n",
      "Processing ArXiv KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 132 nodes, 404 edges.\n",
      "\n",
      "Processing ArXiv KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 110 nodes, 316 edges.\n",
      "[ANCHOR] Using 1 anchor alignments during embedding training.\n",
      "\n",
      "Skipping GAE training; exporting raw node features as embeddings.\n",
      "Saved Wiki entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_wiki.pt\n",
      "Saved ArXiv entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_arxiv.pt\n",
      "  Graph loaded: 110 nodes, 316 edges.\n",
      "[ANCHOR] Using 1 anchor alignments during embedding training.\n",
      "\n",
      "Skipping GAE training; exporting raw node features as embeddings.\n",
      "Saved Wiki entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_wiki.pt\n",
      "Saved ArXiv entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_arxiv.pt\n"
     ]
    }
   ],
   "source": [
    "RUN_EMBEDDINGS = True\n",
    "\n",
    "if RUN_EMBEDDINGS:\n",
    "    print(\"Generating relation embeddings...\")\n",
    "    generate_relation_embeddings()\n",
    "    print(\"\\nGenerating entity embeddings...\")\n",
    "    generate_entity_embeddings()\n",
    "else:\n",
    "    print(\"Skipping embedding generation; using cached tensors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61f0a8",
   "metadata": {},
   "source": [
    "## Section 5: Formulate the problem as a QUBO and solve it\n",
    "Perform the QUBO formulation:\n",
    "$$\n",
    "H_{total} = \\underbrace{\\sum_{i,a}{-S(i, a) \\cdot x_{i,a}}}_{H_{\\text{node}}} + \\underbrace{\\sum_{i,j,a,b}{-w_{ij,ab} \\cdot x_{i,a} \\cdot x_{j,b}}}_{H_{\\text{structure}}} + \\underbrace{\\sum_{i} P_{1} \\sum_{a=1}^M \\sum_{b=a+1}^M x_{i,a} x_{i,b}}_{\\text{Constraint}\\ 1} + \\underbrace{\\sum_{a} P_{2} \\sum_{i=1}^N \\sum_{j=i+1}^N x_{i,a} x_{j,a}}_{\\text{Constraint}\\ 2}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $x_{i,a}$: A binary variable (1 or 0) that is 1 if we align entity $i$ from KG1 with entity $a$ from KG2.\n",
    "  - $S(i,a)$: The similarity score between entity $i$ and $a$, derived from the GAE embeddings.\n",
    "  - $w_{ij,ab}$: The structural similarity weight, derived from the SciBERT relation embeddings.\n",
    "  - $P_1, P_2$: Large positive penalty constants to enforce the constraints.\n",
    "  - Constraint 1: Enforces that each entity $i$ in KG1 maps to at most one entity in KG2. If $i$ matches zero entities, the penalty is 0. If it matches one, the penalty is 0. If it matches two or more, the penalty is high.\n",
    "  - Constraint 2: Enforces that each entity $a$ in KG2 is mapped to by at most one entity from KG1. This allows entities to remain unaligned, making the formulation more robust to realistic KGs that do not have perfect 1-to-1 overlap.\n",
    "\n",
    "And solve it using quantum annealing (more details in the README.md file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aab8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the alignment QUBO...\n",
      "[QUBO] candidate variables: 27, structural pairs: 5\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix.csv with dimension 27×27\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_node.csv with dimension 27×27\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_structure.csv with dimension 27×27\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_penalty.csv with dimension 27×27\n",
      "[QUBO] running simulated annealer with num_reads=100, beta_range=None, seed=None\n",
      "[QUBO] best sample energy=-6.4590 produced 7 alignments\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/aligned_kg.html\n",
      "[QUBO] alignment report saved to /home/nuno/Documents/QUBO-KGA/output/alignments/alignment_annealer.csv with 7 matches and 4 unaligned entries\n"
     ]
    }
   ],
   "source": [
    "SOLVE_QUBO = True\n",
    "\n",
    "result = None\n",
    "if SOLVE_QUBO:\n",
    "    print(\"\\nSolving the alignment QUBO...\")\n",
    "    result = solve_alignment_with_annealer(\n",
    "        similarity_threshold=DEFAULT_SIMILARITY_THRESHOLD,\n",
    "        max_structural_pairs=2000,\n",
    "        visualize=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping QUBO solve; falling back to existing artefacts.\")\n",
    "\n",
    "if result is None:\n",
    "    result = SimpleNamespace(\n",
    "        alignments=[],\n",
    "        energy=float(\"nan\"),\n",
    "        sampleset=None,\n",
    "        aligned_graph_path=KG_ALIGNED_PATH,\n",
    "        aligned_graph_html=(KG_DIR / \"kg_aligned.html\"),\n",
    "        alignment_report_path=ALIGNED_ENTITIES_ANNEALER_CSV,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e3d33",
   "metadata": {},
   "source": [
    "## Section 6: Display HTML Visualizations\n",
    "Render the pruned graphs, aligned knowledge graph, and alignment report directly within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7824dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Aligned Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_entity</th>\n",
       "      <th>arxiv_entity</th>\n",
       "      <th>not_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Post-quantum cryptography</td>\n",
       "      <td>post-quantum cryptography</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantum machine learning</td>\n",
       "      <td>quantum machine learning</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qubit</td>\n",
       "      <td>qubits</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shor's algorithm</td>\n",
       "      <td>Shor's quantum algorithms</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantum optimization algorithms</td>\n",
       "      <td>quantum computing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grover's algorithm</td>\n",
       "      <td>Deutsch's algorithm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quantum computer technology</td>\n",
       "      <td>Grover algorithm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wiki: Noisy intermediate-scale quantum NISQ computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wiki: Peter Shor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv: NISQ devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arxiv: QOA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        wiki_entity               arxiv_entity  \\\n",
       "0         Post-quantum cryptography  post-quantum cryptography   \n",
       "1          quantum machine learning   quantum machine learning   \n",
       "2                             qubit                     qubits   \n",
       "3                  Shor's algorithm  Shor's quantum algorithms   \n",
       "4   Quantum optimization algorithms          quantum computing   \n",
       "5                Grover's algorithm        Deutsch's algorithm   \n",
       "6       quantum computer technology           Grover algorithm   \n",
       "7                               NaN                        NaN   \n",
       "8                               NaN                        NaN   \n",
       "9                               NaN                        NaN   \n",
       "10                              NaN                        NaN   \n",
       "\n",
       "                                              not_aligned  \n",
       "0                                                     NaN  \n",
       "1                                                     NaN  \n",
       "2                                                     NaN  \n",
       "3                                                     NaN  \n",
       "4                                                     NaN  \n",
       "5                                                     NaN  \n",
       "6                                                     NaN  \n",
       "7   wiki: Noisy intermediate-scale quantum NISQ computing  \n",
       "8                                        wiki: Peter Shor  \n",
       "9                                     arxiv: NISQ devices  \n",
       "10                                             arxiv: QOA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aligned_html = KG_DIR / \"kg_aligned.html\"\n",
    "# create the visualizations\n",
    "\n",
    "visualize_ttl(KG_ALIGNED_PATH, aligned_html)\n",
    "\n",
    "_open_in_browser(aligned_html, \"Aligned Knowledge Graph\")\n",
    "\n",
    "# # Display only the DataFrame in the notebook\n",
    "display(pd.read_csv(ALIGNED_ENTITIES_ANNEALER_CSV))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGA (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
