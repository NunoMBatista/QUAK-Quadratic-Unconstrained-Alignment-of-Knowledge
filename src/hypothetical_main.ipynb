{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3f91fc",
   "metadata": {},
   "source": [
    "# Hypothetical Hybrid GNN-QUBO Alignment Pipeline\n",
    "\n",
    "Keep in mind that this Jupyter notebook's purpose is not to show an advantage of the QUBO method over the NN method. \n",
    "\n",
    "This is just a proposed pipeline for QUBO re-ranking. \n",
    "\n",
    "It implements all of the steps, from data fetching to the QUBO solver.\n",
    "\n",
    "We start by building two regular knowledge graphs (unpruned) and then prune them to simulate how a smaller knowledge graph would look like (in theory, that smaller knowledge graph would contain the ambiguous entities which didn't get a high alignment confidence score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6368166",
   "metadata": {},
   "source": [
    "## Section 1: Load Project Dependencies\n",
    "- Set up the Python path for the project and import every module that the pipeline relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407be95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuno/python_envs/KGA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "import webbrowser\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, IFrame, display\n",
    "\n",
    "repo_root = Path().resolve().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from src.config import *\n",
    "from src.evaluation.solvers import (\n",
    "    solve_alignment_with_annealer,\n",
    "    solve_alignment_with_nearest_neighbor,\n",
    " )\n",
    "from src.kg_construction.fetch_data import fetch_wiki_data, fetch_arxiv_data\n",
    "from src.kg_construction.build_kg import build_unpruned_kgs, prune_kgs\n",
    "from src.embedding.generate_embeddings import (\n",
    "    generate_relation_embeddings,\n",
    "    generate_entity_embeddings,\n",
    " )\n",
    "from src.utils.graph_visualizer import visualize_ttl\n",
    "\n",
    "# [FIX THIS LATER] maintain backward-compatible variable name for existing cells\n",
    "ALIGNED_ENTITIES_CSV = ALIGNED_ENTITIES_ANNEALER_CSV\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# create directories if they don't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WIKI_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARXIV_ENTITIES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _open_in_browser(path, title):\n",
    "    \"\"\"Open a local HTML file in the default browser.\"\"\"\n",
    "    path = Path(path)\n",
    "    webbrowser.open(f\"file://{path.resolve()}\")\n",
    "    display(HTML(f\"<p><i>Opening '{title}' in the browser...</i></p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc7687",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation\n",
    "- Fetch the Wikipedia and arXiv raw data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58de790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching source corpora...\n",
      "-> all requested Wikipedia articles already cached; reusing local files\n",
      "    -> returning 10 Wikipedia summaries.\n",
      "-> all requested arXiv abstracts already cached; reusing local files\n",
      "    -> returning 10 arXiv abstracts.\n",
      "Wikipedia titles: ['Quantum algorithm', 'Post-quantum cryptography', 'Quantum optimization algorithms', 'Quantum computing', \"Shor's algorithm\", \"Grover's algorithm\", 'Noisy intermediate-scale quantum computing', 'Quantum machine learning', 'Quantum counting algorithm', 'Quantum phase estimation algorithm']\n",
      "arXiv IDs: ['2310.03011v2', '2406.13258v3', '2312.13636v3', '0708.0261v1', 'quant-ph/9508027v2', '2108.10854v2', '1801.00862v3', '1611.09347v2', 'quant-ph/9805082v1', 'quant-ph/9511026v1']\n"
     ]
    }
   ],
   "source": [
    "wiki_titles = []\n",
    "arxiv_ids = []\n",
    "\n",
    "\n",
    "print(\"Fetching source corpora...\")\n",
    "wiki_summaries, wiki_titles = fetch_wiki_data()\n",
    "arxiv_abstracts, arxiv_ids = fetch_arxiv_data()\n",
    "print(f\"Wikipedia titles: {wiki_titles}\")\n",
    "print(f\"arXiv IDs: {arxiv_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df661f0",
   "metadata": {},
   "source": [
    "# Section 3: Build the KGs.\n",
    "\n",
    "- Run the NLP pipeline to perform Named Entity Recognition (NER) and Relationship Extraction (RE) on the raw data.\n",
    "\n",
    "- Use the entities and the relations between them to build two large unpruned graphs.\n",
    "\n",
    "- Take the unpruned graphs and reduce them to just a couple entities. In practice, those would be tha \"ambiguous\" entities whose alignment confidence score is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a69849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building and pruning knowledge graphs...\n",
      "\n",
      "--- STEP 1: SKIPPING BUILD; UNPRUNED TTLs ARE ALREADY PRESENT ---\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/unpruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Unpruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nBuilding and pruning knowledge graphs...\")\n",
    "build_unpruned_kgs(\n",
    "    wiki_data=wiki_summaries, \n",
    "    arxiv_data=arxiv_abstracts\n",
    ")\n",
    "\n",
    "visualize_ttl(KG_WIKI_UNPRUNED_PATH, KG_DIR / \"unpruned_wiki_kg.html\")\n",
    "visualize_ttl(KG_ARXIV_UNPRUNED_PATH, KG_DIR / \"unpruned_arxiv_kg.html\")\n",
    "\n",
    "\n",
    "_open_in_browser(KG_DIR / \"unpruned_wiki_kg.html\", \"Unpruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(KG_DIR / \"unpruned_arxiv_kg.html\", \"Unpruned arXiv Knowledge Graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5f5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 4: PRUNING KGs ---\n",
      "\n",
      "-> pruning Wiki KG...\n",
      "    -> loaded 151 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 16 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "-> pruning arXiv KG...\n",
      "    -> loaded 101 raw triples, pruning with 9 entities.\n",
      "    -> Found and added 9 matching entities.\n",
      "    -> Saving 10 clean triples to /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_wiki_kg.html\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_final.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/pruned_arxiv_kg.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned Wiki Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Pruned arXiv Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prune_kgs()\n",
    "\n",
    "wiki_html = KG_DIR / \"pruned_wiki_kg.html\"\n",
    "arxiv_html = KG_DIR / \"pruned_arxiv_kg.html\"\n",
    "visualize_ttl(KG_WIKI_FINAL_PATH, wiki_html)\n",
    "visualize_ttl(KG_ARXIV_FINAL_PATH, arxiv_html)\n",
    "\n",
    "# Open HTML files in browser\n",
    "_open_in_browser(wiki_html, \"Pruned Wiki Knowledge Graph\")\n",
    "_open_in_browser(arxiv_html, \"Pruned arXiv Knowledge Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886dae0",
   "metadata": {},
   "source": [
    "## Section 4: Generate Embeddings\n",
    "Generate the following embeddings:\n",
    "- Entity embeddings (using a GAE that fine-tunes the SciBERT embeddings)\n",
    "- Relation embeddings (using the SciBERT embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50efc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating relation embeddings...\n",
      "\n",
      "--- Part 1: Generating Relation Embeddings (for H_structure) ---\n",
      "Loading SciBERT model: allenai/scibert_scivocab_cased...\n",
      "Discovered 166 relation labels.\n",
      "  - able to simulate\n",
      "  - achieves quadratic speedup over\n",
      "  - affected by\n",
      "  - aids\n",
      "  - also known as\n",
      "  - amplify\n",
      "  - analyze\n",
      "  - applied on\n",
      "  - applies to\n",
      "  - approximates execution of\n",
      "  - based on\n",
      "  - become\n",
      "  - become fully fault tolerant\n",
      "  - believed to be\n",
      "  - can be performed on\n",
      "  - can brute force\n",
      "  - can exist in\n",
      "  - can speed up\n",
      "  - cannot be efficiently simulated on\n",
      "  - characterized by\n",
      "  - coined by\n",
      "  - combine with\n",
      "  - combined with\n",
      "  - compare\n",
      "  - compare against\n",
      "  - considers level\n",
      "  - contain\n",
      "  - contains\n",
      "  - could break\n",
      "  - could perform exponentially faster calculations than\n",
      "  - counts solutions for\n",
      "  - deals with\n",
      "  - defined by\n",
      "  - demonstrated in\n",
      "  - describe\n",
      "  - designing\n",
      "  - developed by\n",
      "  - development of\n",
      "  - devised\n",
      "  - devised by\n",
      "  - discuss\n",
      "  - does not require\n",
      "  - enable\n",
      "  - enables\n",
      "  - enter stage\n",
      "  - estimates\n",
      "  - evaluate possible\n",
      "  - examine\n",
      "  - executed faster on\n",
      "  - exploits\n",
      "  - explores\n",
      "  - extends\n",
      "  - faster than\n",
      "  - finds\n",
      "  - finds unique input of\n",
      "  - formulated as\n",
      "  - generalized by\n",
      "  - given for\n",
      "  - hard on\n",
      "  - has complexity\n",
      "  - has query complexity\n",
      "  - has similarity to\n",
      "  - have\n",
      "  - have advantage over\n",
      "  - have over\n",
      "  - helps\n",
      "  - hosted since\n",
      "  - implemented on\n",
      "  - improve complexity\n",
      "  - improve complexity of\n",
      "  - improves efficiency of\n",
      "  - in complexity class\n",
      "  - includes\n",
      "  - incorporate\n",
      "  - independent of\n",
      "  - inefficiently produce\n",
      "  - instance of\n",
      "  - intersects\n",
      "  - introduced by\n",
      "  - introduced in\n",
      "  - investigate\n",
      "  - involve\n",
      "  - is a\n",
      "  - is state of\n",
      "  - lack processing power to break\n",
      "  - limited by\n",
      "  - limits\n",
      "  - manipulates\n",
      "  - may not increase risk to\n",
      "  - may outperform\n",
      "  - measured by\n",
      "  - more complex than classical\n",
      "  - needed for\n",
      "  - not advanced for\n",
      "  - not capable of\n",
      "  - not large enough for\n",
      "  - not practical for\n",
      "  - obtainable for\n",
      "  - offers\n",
      "  - opened paths towards\n",
      "  - operates on\n",
      "  - outsourced to\n",
      "  - passed by\n",
      "  - performs\n",
      "  - pioneered by\n",
      "  - prepare for\n",
      "  - presents\n",
      "  - produce\n",
      "  - prone to\n",
      "  - proposed\n",
      "  - proved lower bound\n",
      "  - provide insight into\n",
      "  - provides\n",
      "  - provides speedup\n",
      "  - provides speedup in\n",
      "  - provides speedup over\n",
      "  - realizes\n",
      "  - reduce security of\n",
      "  - refer to\n",
      "  - related to\n",
      "  - rely on\n",
      "  - remain norm\n",
      "  - remains undecidable using\n",
      "  - requires\n",
      "  - requires evaluations\n",
      "  - requires exponential energy for\n",
      "  - requires exponential time for\n",
      "  - run on\n",
      "  - runs\n",
      "  - runs faster than\n",
      "  - runs in time\n",
      "  - runs on\n",
      "  - samples from\n",
      "  - scope restricted to\n",
      "  - secure against\n",
      "  - sensitive to\n",
      "  - serves as subroutine in\n",
      "  - serves same function as\n",
      "  - solution to\n",
      "  - solvable by\n",
      "  - solve\n",
      "  - solves\n",
      "  - solves faster than\n",
      "  - span across\n",
      "  - strive for\n",
      "  - studies\n",
      "  - subclass of\n",
      "  - subtype of\n",
      "  - suffers from\n",
      "  - suggest speedup over\n",
      "  - surpasses\n",
      "  - synonym of\n",
      "  - takes steps polynomial in\n",
      "  - transform\n",
      "  - use\n",
      "  - used as basis of\n",
      "  - used as subroutine in\n",
      "  - used for\n",
      "  - useful for exploring\n",
      "  - uses\n",
      "  - uses algorithm\n",
      "  - uses complexity\n",
      "  - utilizes\n",
      "  - will be available in\n",
      "  - will not change world right away\n",
      "  - yields state via\n",
      "Successfully saved relation embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/relation_embeddings.npz\n",
      "\n",
      "Generating entity embeddings...\n",
      "\n",
      "--- Part 2: Generating Entity Embeddings (for H_node) ---\n",
      "\n",
      "Processing Wiki KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_wiki_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 130 nodes, 410 edges.\n",
      "\n",
      "Processing ArXiv KG:\n",
      "  Loading graph from /home/nuno/Documents/QUBO-KGA/output/KGs/kg_arxiv_unpruned.ttl...\n",
      "  Generating SciBERT features for nodes...\n",
      "  Graph loaded: 99 nodes, 299 edges.\n",
      "\n",
      "Skipping GAE training; exporting raw node features as embeddings.\n",
      "Saved Wiki entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_wiki.pt\n",
      "Saved ArXiv entity embeddings to /home/nuno/Documents/QUBO-KGA/output/embeddings/entity_embeddings_arxiv.pt\n"
     ]
    }
   ],
   "source": [
    "RUN_EMBEDDINGS = True\n",
    "\n",
    "if RUN_EMBEDDINGS:\n",
    "    print(\"Generating relation embeddings...\")\n",
    "    generate_relation_embeddings()\n",
    "    print(\"\\nGenerating entity embeddings...\")\n",
    "    generate_entity_embeddings()\n",
    "else:\n",
    "    print(\"Skipping embedding generation; using cached tensors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61f0a8",
   "metadata": {},
   "source": [
    "## Section 5: Formulate the problem as a QUBO and solve it\n",
    "Perform the QUBO formulation:\n",
    "$$\n",
    "H_{total} = \\underbrace{\\sum_{i,a}{-S(i, a) \\cdot x_{i,a}}}_{H_{\\text{node}}} + \\underbrace{\\sum_{i,j,a,b}{-w_{ij,ab} \\cdot x_{i,a} \\cdot x_{j,b}}}_{H_{\\text{structure}}} + \\underbrace{\\sum_{i} P_{1} \\sum_{a=1}^M \\sum_{b=a+1}^M x_{i,a} x_{i,b}}_{\\text{Constraint}\\ 1} + \\underbrace{\\sum_{a} P_{2} \\sum_{i=1}^N \\sum_{j=i+1}^N x_{i,a} x_{j,a}}_{\\text{Constraint}\\ 2}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $x_{i,a}$: A binary variable (1 or 0) that is 1 if we align entity $i$ from KG1 with entity $a$ from KG2.\n",
    "  - $S(i,a)$: The similarity score between entity $i$ and $a$, derived from the GAE embeddings.\n",
    "  - $w_{ij,ab}$: The structural similarity weight, derived from the SciBERT relation embeddings.\n",
    "  - $P_1, P_2$: Large positive penalty constants to enforce the constraints.\n",
    "  - Constraint 1: Enforces that each entity $i$ in KG1 maps to at most one entity in KG2. If $i$ matches zero entities, the penalty is 0. If it matches one, the penalty is 0. If it matches two or more, the penalty is high.\n",
    "  - Constraint 2: Enforces that each entity $a$ in KG2 is mapped to by at most one entity from KG1. This allows entities to remain unaligned, making the formulation more robust to realistic KGs that do not have perfect 1-to-1 overlap.\n",
    "\n",
    "And solve it using quantum annealing (more details in the README.md file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aab8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving the alignment QUBO...\n",
      "[QUBO] candidate variables: 81, structural pairs: 6\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix.csv with dimension 81×81\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_node.csv with dimension 81×81\n",
      "[QUBO] matrix exported to /home/nuno/Documents/QUBO-KGA/output/qubo/qubo_matrix_H_structure.csv with dimension 81×81\n",
      "[QUBO] running simulated annealer with num_reads=100, beta_range=None, seed=None\n",
      "[QUBO] best sample energy=-8.3335 produced 9 alignments\n",
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/aligned_kg.html\n",
      "[QUBO] alignment report saved to /home/nuno/Documents/QUBO-KGA/output/alignments/alignment_annealer.csv with 9 matches and 0 unaligned entries\n"
     ]
    }
   ],
   "source": [
    "SOLVE_QUBO = True\n",
    "\n",
    "result = None\n",
    "if SOLVE_QUBO:\n",
    "    print(\"\\nSolving the alignment QUBO...\")\n",
    "    result = solve_alignment_with_annealer(\n",
    "        similarity_threshold=0.0,\n",
    "        max_structural_pairs=2000,\n",
    "        visualize=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping QUBO solve; falling back to existing artefacts.\")\n",
    "\n",
    "if result is None:\n",
    "    result = SimpleNamespace(\n",
    "        alignments=[],\n",
    "        energy=float(\"nan\"),\n",
    "        sampleset=None,\n",
    "        aligned_graph_path=KG_ALIGNED_PATH,\n",
    "        aligned_graph_html=(KG_DIR / \"kg_aligned.html\"),\n",
    "        alignment_report_path=ALIGNED_ENTITIES_ANNEALER_CSV,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e3d33",
   "metadata": {},
   "source": [
    "## Section 6: Display HTML Visualizations\n",
    "Render the pruned graphs, aligned knowledge graph, and alignment report directly within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7824dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph from: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.ttl\n",
      "\n",
      "saved graph visualization to: /home/nuno/Documents/QUBO-KGA/output/KGs/kg_aligned.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><i>Opening 'Aligned Knowledge Graph' in the browser...</i></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki_entity</th>\n",
       "      <th>arxiv_entity</th>\n",
       "      <th>not_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quantum optimization algorithms</td>\n",
       "      <td>quantum optimization algorithms</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quantum computer</td>\n",
       "      <td>quantum computing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantum computing</td>\n",
       "      <td>quantum computer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noisy intermediate-scale quantum NISQ computing</td>\n",
       "      <td>post-quantum cryptography PQC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantum machine learning QML</td>\n",
       "      <td>quantum machine learning</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shor's algorithm</td>\n",
       "      <td>Grover algorithm</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Post-quantum cryptography</td>\n",
       "      <td>NISQ devices</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Peter Shor</td>\n",
       "      <td>Shor's quantum algorithms</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Grover's algorithm</td>\n",
       "      <td>QOA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       wiki_entity  \\\n",
       "0                  Quantum optimization algorithms   \n",
       "1                                 Quantum computer   \n",
       "2                                Quantum computing   \n",
       "3  Noisy intermediate-scale quantum NISQ computing   \n",
       "4                     Quantum machine learning QML   \n",
       "5                                 Shor's algorithm   \n",
       "6                        Post-quantum cryptography   \n",
       "7                                       Peter Shor   \n",
       "8                               Grover's algorithm   \n",
       "\n",
       "                      arxiv_entity  not_aligned  \n",
       "0  quantum optimization algorithms          NaN  \n",
       "1                quantum computing          NaN  \n",
       "2                 quantum computer          NaN  \n",
       "3    post-quantum cryptography PQC          NaN  \n",
       "4         quantum machine learning          NaN  \n",
       "5                 Grover algorithm          NaN  \n",
       "6                     NISQ devices          NaN  \n",
       "7        Shor's quantum algorithms          NaN  \n",
       "8                              QOA          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aligned_html = KG_DIR / \"kg_aligned.html\"\n",
    "# create the visualizations\n",
    "\n",
    "visualize_ttl(KG_ALIGNED_PATH, aligned_html)\n",
    "\n",
    "_open_in_browser(aligned_html, \"Aligned Knowledge Graph\")\n",
    "\n",
    "# # Display only the DataFrame in the notebook\n",
    "display(pd.read_csv(ALIGNED_ENTITIES_ANNEALER_CSV))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
